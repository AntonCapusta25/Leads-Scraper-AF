#!/usr/bin/env python3

"""

Professional Contact & People Scraper

Built with DrissionPage - Headless Mode

API-Ready for Frontend Integration

"""



import asyncio

import time

import random

import re

import logging

from typing import Dict, List, Any, Optional, Tuple, Set

from urllib.parse import urljoin, urlparse, quote

import json

from dataclasses import dataclass, asdict

from datetime import datetime

import hashlib

from difflib import SequenceMatcher

from collections import defaultdict

import globÂ  Â  Â  Â  # Add this for wildcard path matching

import uuidÂ  Â  Â  Â  # Add this for random directories

# API Framework

from fastapi import FastAPI, HTTPException, BackgroundTasks

from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel, Field

import uvicorn



# Railway environment configuration

import os

PORT = int(os.getenv("PORT", 8000))

RAILWAY_ENV = os.getenv("RAILWAY_ENVIRONMENT_NAME", "production")



# Configure logging for Railway

logging.basicConfig(

Â  Â  level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),

Â  Â  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'

)

logger = logging.getLogger(__name__)



@dataclass

class SearchParameters:

Â  Â  """Search parameters for contact scraping"""

Â  Â  industry: Optional[str] = None

Â  Â  position: Optional[str] = None

Â  Â  company: Optional[str] = None

Â  Â  country: Optional[str] = None

Â  Â  city: Optional[str] = None

Â  Â  keywords: Optional[str] = None

Â  Â  experience_level: Optional[str] = NoneÂ  # junior, senior, manager, director, etc.

Â  Â  company_size: Optional[str] = NoneÂ  # startup, small, medium, large, enterprise

Â  Â  max_results: int = 50

Â  Â Â 

Â  Â  def to_search_string(self) -> str:

Â  Â  Â  Â  """Convert parameters to search string"""

Â  Â  Â  Â  parts = []

Â  Â  Â  Â  if self.position: parts.append(f'"{self.position}"')

Â  Â  Â  Â  if self.industry: parts.append(f'"{self.industry}"')

Â  Â  Â  Â  if self.company: parts.append(f'"{self.company}"')

Â  Â  Â  Â  if self.country: parts.append(self.country)

Â  Â  Â  Â  if self.city: parts.append(self.city)

Â  Â  Â  Â  if self.keywords: parts.append(self.keywords)

Â  Â  Â  Â  return " ".join(parts)



@dataclass

class ContactResult:

Â  Â  """Contact result data structure"""

Â  Â  name: str

Â  Â  position: Optional[str] = None

Â  Â  company: Optional[str] = None

Â  Â  location: Optional[str] = None

Â  Â  email: Optional[str] = None

Â  Â  phone: Optional[str] = None

Â  Â  linkedin_url: Optional[str] = None

Â  Â  profile_url: Optional[str] = None

Â  Â  industry: Optional[str] = None

Â  Â  experience: Optional[str] = None

Â  Â  summary: Optional[str] = None

Â  Â  source: Optional[str] = None

Â  Â  scraped_at: Optional[str] = None

Â  Â  confidence_score: float = 0.0

Â  Â Â 

Â  Â  def __post_init__(self):

Â  Â  Â  Â  if not self.scraped_at:

Â  Â  Â  Â  Â  Â  self.scraped_at = datetime.now().isoformat()



class ContactScraper:

Â  Â  """Professional Contact Scraper using DrissionPage"""

Â  Â Â 

Â  Â  def __init__(self, headless: bool = True, stealth: bool = True, dedup_strictness: str = "medium"):

Â  Â  Â  Â  self.browser_page = None

Â  Â  Â  Â  self.session_page = None

Â  Â  Â  Â  self.headless = headless

Â  Â  Â  Â  self.stealth = stealth

Â  Â  Â  Â  self._browser_created = False

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Deduplication settings

Â  Â  Â  Â  self.dedup_strictness = dedup_strictness

Â  Â  Â  Â  self.dedup_thresholds = self._get_dedup_thresholds(dedup_strictness)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # User agents for rotation

Â  Â  Â  Â  self.user_agents = [

Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',

Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',

Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Rate limiting - more conservative for LinkedIn

Â  Â  Â  Â  self.last_request_time = 0

Â  Â  Â  Â  self.min_delay = 2Â  # Minimum delay between requests

Â  Â  Â  Â  self.linkedin_delay = 5Â  # Longer delay for LinkedIn

Â  Â  Â  Â  self.linkedin_request_count = 0

Â  Â  Â  Â  self.linkedin_max_requests_per_hour = 20Â  # Very conservative

Â  Â Â 

Â  Â  def _get_dedup_thresholds(self, strictness: str) -> Dict[str, float]:

Â  Â  Â  Â  """Get deduplication thresholds based on strictness level"""

Â  Â  Â  Â  if strictness == "loose":

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "name_similarity": 0.7,

Â  Â  Â  Â  Â  Â  Â  Â  "company_similarity": 0.6,

Â  Â  Â  Â  Â  Â  Â  Â  "name_company_threshold": 0.65,

Â  Â  Â  Â  Â  Â  Â  Â  "position_similarity": 0.6,

Â  Â  Â  Â  Â  Â  Â  Â  "location_similarity": 0.5,

Â  Â  Â  Â  Â  Â  Â  Â  "email_domain_name_threshold": 0.6

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  elif strictness == "strict":

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "name_similarity": 0.95,

Â  Â  Â  Â  Â  Â  Â  Â  "company_similarity": 0.95,

Â  Â  Â  Â  Â  Â  Â  Â  "name_company_threshold": 0.95,

Â  Â  Â  Â  Â  Â  Â  Â  "position_similarity": 0.9,

Â  Â  Â  Â  Â  Â  Â  Â  "location_similarity": 0.9,

Â  Â  Â  Â  Â  Â  Â  Â  "email_domain_name_threshold": 0.9

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  else:Â  # medium (default)

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "name_similarity": 0.85,

Â  Â  Â  Â  Â  Â  Â  Â  "company_similarity": 0.9,

Â  Â  Â  Â  Â  Â  Â  Â  "name_company_threshold": 0.85,

Â  Â  Â  Â  Â  Â  Â  Â  "position_similarity": 0.8,

Â  Â  Â  Â  Â  Â  Â  Â  "location_similarity": 0.7,

Â  Â  Â  Â  Â  Â  Â  Â  "email_domain_name_threshold": 0.8

Â  Â  Â  Â  Â  Â  }

Â  Â Â 

Â  Â  def set_dedup_strictness(self, strictness: str):

Â  Â  Â  Â  """Update deduplication strictness"""

Â  Â  Â  Â  self.dedup_strictness = strictness

Â  Â  Â  Â  self.dedup_thresholds = self._get_dedup_thresholds(strictness)

Â  Â  Â  Â  logger.info(f"ğŸ“Š Deduplication strictness set to: {strictness}")

Â  Â Â 

Â  Â  async def deduplicate_contacts(self, contacts: List[ContactResult], strictness: str = None) -> Tuple[List[ContactResult], Dict[str, Any]]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Public method to deduplicate contacts with statistics

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if strictness:

Â  Â  Â  Â  Â  Â  Â  Â  old_strictness = self.dedup_strictness

Â  Â  Â  Â  Â  Â  Â  Â  self.set_dedup_strictness(strictness)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  original_count = len(contacts)

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”„ Starting deduplication of {original_count} contacts with {self.dedup_strictness} strictness")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Perform deduplication

Â  Â  Â  Â  Â  Â  deduplicated_contacts = self._deduplicate_results(contacts)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Generate statistics

Â  Â  Â  Â  Â  Â  stats = self._generate_dedup_stats(contacts, deduplicated_contacts)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if strictness:

Â  Â  Â  Â  Â  Â  Â  Â  # Restore original strictness

Â  Â  Â  Â  Â  Â  Â  Â  self.set_dedup_strictness(old_strictness)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return deduplicated_contacts, stats

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Contact deduplication failed: {e}")

Â  Â  Â  Â  Â  Â  return contacts, {"error": str(e)}

Â  Â Â 

Â  Â  def _generate_dedup_stats(self, original: List[ContactResult], deduplicated: List[ContactResult]) -> Dict[str, Any]:

Â  Â  Â  Â  """Generate deduplication statistics"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  original_count = len(original)

Â  Â  Â  Â  Â  Â  deduplicated_count = len(deduplicated)

Â  Â  Â  Â  Â  Â  removed_count = original_count - deduplicated_count

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Analyze what types of duplicates were found

Â  Â  Â  Â  Â  Â  duplicate_types = {

Â  Â  Â  Â  Â  Â  Â  Â  "email_duplicates": 0,

Â  Â  Â  Â  Â  Â  Â  Â  "linkedin_duplicates": 0,

Â  Â  Â  Â  Â  Â  Â  Â  "name_company_duplicates": 0,

Â  Â  Â  Â  Â  Â  Â  Â  "phone_duplicates": 0,

Â  Â  Â  Â  Â  Â  Â  Â  "fuzzy_matches": 0

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Count sources

Â  Â  Â  Â  Â  Â  source_distribution = defaultdict(int)

Â  Â  Â  Â  Â  Â  for contact in deduplicated:

Â  Â  Â  Â  Â  Â  Â  Â  if contact.source:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source_distribution[contact.source] += 1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Confidence score distribution

Â  Â  Â  Â  Â  Â  confidence_buckets = {"high": 0, "medium": 0, "low": 0}

Â  Â  Â  Â  Â  Â  for contact in deduplicated:

Â  Â  Â  Â  Â  Â  Â  Â  if contact.confidence_score >= 0.8:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_buckets["high"] += 1

Â  Â  Â  Â  Â  Â  Â  Â  elif contact.confidence_score >= 0.5:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_buckets["medium"] += 1

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_buckets["low"] += 1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "original_count": original_count,

Â  Â  Â  Â  Â  Â  Â  Â  "deduplicated_count": deduplicated_count,

Â  Â  Â  Â  Â  Â  Â  Â  "duplicates_removed": removed_count,

Â  Â  Â  Â  Â  Â  Â  Â  "duplicate_rate": round((removed_count / original_count) * 100, 2) if original_count > 0 else 0,

Â  Â  Â  Â  Â  Â  Â  Â  "strictness_used": self.dedup_strictness,

Â  Â  Â  Â  Â  Â  Â  Â  "duplicate_types": duplicate_types,

Â  Â  Â  Â  Â  Â  Â  Â  "source_distribution": dict(source_distribution),

Â  Â  Â  Â  Â  Â  Â  Â  "confidence_distribution": confidence_buckets,

Â  Â  Â  Â  Â  Â  Â  Â  "thresholds_used": self.dedup_thresholds

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Stats generation failed: {e}")

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "original_count": len(original),

Â  Â  Â  Â  Â  Â  Â  Â  "deduplicated_count": len(deduplicated),

Â  Â  Â  Â  Â  Â  Â  Â  "error": str(e)

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  def _safe_import_drissionpage(self):

Â  Â  Â  Â  """Safely import DrissionPage"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  from DrissionPage import ChromiumPage, ChromiumOptions, SessionPage

Â  Â  Â  Â  Â  Â  logger.info("âœ… DrissionPage imported successfully")

Â  Â  Â  Â  Â  Â  return ChromiumPage, ChromiumOptions, SessionPage, True

Â  Â  Â  Â  except ImportError as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ DrissionPage not available: {e}")

Â  Â  Â  Â  Â  Â  logger.error("Install with: pip install DrissionPage")

Â  Â  Â  Â  Â  Â  return None, None, None, False

Â  Â Â 

Â  Â  def _create_stealth_options(self):

Â  Â  Â  Â  """Create enhanced stealth browser options for Railway"""

Â  Â  Â  Â  ChromiumPage, ChromiumOptions, SessionPage, available = self._safe_import_drissionpage()

Â  Â  Â  Â Â 

Â  Â  Â  Â  if not available:

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  co = ChromiumOptions()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Railway Chrome detection

Â  Â  Â  Â  Â  Â  chrome_found = False

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Try environment variables first

Â  Â  Â  Â  Â  Â  chrome_env_path = os.getenv('CHROME_BIN') or os.getenv('CHROMIUM_PATH')

Â  Â  Â  Â  Â  Â  if chrome_env_path:

Â  Â  Â  Â  Â  Â  Â  Â  # Handle wildcard paths

Â  Â  Â  Â  Â  Â  Â  Â  if '*' in chrome_env_path:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import glob

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matches = glob.glob(chrome_env_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  actual_path = matches[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists(actual_path):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  co.set_browser_path(actual_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chrome_found = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found Chrome via env var: {actual_path}")

Â  Â  Â  Â  Â  Â  Â  Â  elif os.path.exists(chrome_env_path):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  co.set_browser_path(chrome_env_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chrome_found = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found Chrome via env var: {chrome_env_path}")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Try standard paths if env vars don't work

Â  Â  Â  Â  Â  Â  if not chrome_found:

Â  Â  Â  Â  Â  Â  Â  Â  chrome_paths = [

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '/nix/store/*/bin/chromium',Â  Â  Â  # Railway/Nix

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '/usr/bin/chromium',Â  Â  Â  Â  Â  Â  Â  # Standard

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '/usr/bin/chromium-browser',Â  Â  Â  # Alternative

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '/usr/bin/google-chrome-stable',Â  # Google Chrome

Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  for path in chrome_paths:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if '*' in path:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import glob

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matches = glob.glob(path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  actual_path = matches[0]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists(actual_path):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  co.set_browser_path(actual_path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chrome_found = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found Chrome: {actual_path}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif os.path.exists(path):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  co.set_browser_path(path)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chrome_found = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found Chrome: {path}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not chrome_found:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸ Chrome not found - letting DrissionPage auto-detect")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Railway-optimized arguments

Â  Â  Â  Â  Â  Â  if self.headless:

Â  Â  Â  Â  Â  Â  Â  Â  co.set_argument('--headless=new')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  railway_args = [

Â  Â  Â  Â  Â  Â  Â  Â  '--no-sandbox',Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Required for Railway

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-dev-shm-usage',Â  Â  Â  Â  Â # Required for Railway

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-gpu',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-web-security',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-extensions',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-plugins',Â 

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-images',

Â  Â  Â  Â  Â  Â  Â  Â  '--no-first-run',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-infobars',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-notifications',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-popup-blocking',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-automation',

Â  Â  Â  Â  Â  Â  Â  Â  '--disable-blink-features=AutomationControlled',

Â  Â  Â  Â  Â  Â  Â  Â  '--window-size=1920,1080',

Â  Â  Â  Â  Â  Â  Â  Â  '--user-agent=' + random.choice(self.user_agents)

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for arg in railway_args:

Â  Â  Â  Â  Â  Â  Â  Â  co.set_argument(arg)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Set temp directory

Â  Â  Â  Â  Â  Â  co.set_user_data_path(f'/tmp/chrome_{uuid.uuid4().hex[:8]}')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return co

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Chrome options failed: {e}")

Â  Â  Â  Â  Â  Â  return None



Â  Â  async def _create_browser(self):

Â  Â  Â  Â  """Create browser instance with enhanced error handling"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if self._browser_created:

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  ChromiumPage, ChromiumOptions, SessionPage, available = self._safe_import_drissionpage()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not available:

Â  Â  Â  Â  Â  Â  Â  Â  logger.error("âŒ DrissionPage not available - cannot create browser")

Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Create browser options

Â  Â  Â  Â  Â  Â  options = self._create_stealth_options()

Â  Â  Â  Â  Â  Â  if not options:

Â  Â  Â  Â  Â  Â  Â  Â  logger.error("âŒ Failed to create browser options")

Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Create browser page

Â  Â  Â  Â  Â  Â  logger.info("ğŸŒ Creating browser instance...")

Â  Â  Â  Â  Â  Â  self.browser_page = ChromiumPage(addr_or_opts=options)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Test browser

Â  Â  Â  Â  Â  Â  await asyncio.sleep(1)

Â  Â  Â  Â  Â  Â  self.browser_page.get("https://httpbin.org/user-agent")

Â  Â  Â  Â  Â  Â  await asyncio.sleep(2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  self._browser_created = True

Â  Â  Â  Â  Â  Â  logger.info("âœ… Browser created successfully")

Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Browser creation failed: {e}")

Â  Â  Â  Â  Â  Â  self.browser_page = None

Â  Â  Â  Â  Â  Â  self._browser_created = False

Â  Â  Â  Â  Â  Â  return False

Â  Â Â 

Â  Â  async def _rate_limit(self):

Â  Â  Â  Â  """Apply standard rate limiting"""

Â  Â  Â  Â  current_time = time.time()

Â  Â  Â  Â  time_since_last = current_time - self.last_request_time

Â  Â  Â  Â Â 

Â  Â  Â  Â  if time_since_last < self.min_delay:

Â  Â  Â  Â  Â  Â  sleep_time = self.min_delay - time_since_last + random.uniform(0.5, 1.5)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(sleep_time)

Â  Â  Â  Â Â 

Â  Â  Â  Â  self.last_request_time = time.time()

Â  Â Â 

Â  Â  async def _rate_limit_linkedin(self):

Â  Â  Â  Â  """Apply enhanced rate limiting for LinkedIn"""

Â  Â  Â  Â  current_time = time.time()

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Check hourly rate limit

Â  Â  Â  Â  self.linkedin_request_count += 1

Â  Â  Â  Â  if self.linkedin_request_count > self.linkedin_max_requests_per_hour:

Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸ LinkedIn hourly rate limit reached. Consider using LinkedIn API.")

Â  Â  Â  Â  Â  Â  await asyncio.sleep(3600)Â  # Wait an hour

Â  Â  Â  Â  Â  Â  self.linkedin_request_count = 0

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Apply longer delay for LinkedIn

Â  Â  Â  Â  time_since_last = current_time - self.last_request_time

Â  Â  Â  Â  if time_since_last < self.linkedin_delay:

Â  Â  Â  Â  Â  Â  sleep_time = self.linkedin_delay - time_since_last + random.uniform(2, 5)

Â  Â  Â  Â  Â  Â  logger.info(f"â³ LinkedIn rate limiting: waiting {sleep_time:.1f}s")

Â  Â  Â  Â  Â  Â  await asyncio.sleep(sleep_time)

Â  Â  Â  Â Â 

Â  Â  Â  Â  self.last_request_time = time.time()

Â  Â Â 

Â  Â  async def search_linkedin_api_alternative(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Alternative method using LinkedIn API (requires API key)

Â  Â  Â  Â  This is the RECOMMENDED approach for production use

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  logger.info("ğŸ“¡ Using LinkedIn API (recommended method)")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # This would require LinkedIn API credentials

Â  Â  Â  Â  Â  Â  # Placeholder for LinkedIn API integration

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Example API call structure:

Â  Â  Â  Â  Â  Â  # headers = {

Â  Â  Â  Â  Â  Â  #Â  Â  Â 'Authorization': f'Bearer {linkedin_api_token}',

Â  Â  Â  Â  Â  Â  #Â  Â  Â 'Content-Type': 'application/json'

Â  Â  Â  Â  Â  Â  # }

Â  Â  Â  Â  Â  Â  #Â 

Â  Â  Â  Â  Â  Â  # api_url = 'https://api.linkedin.com/v2/people-search'

Â  Â  Â  Â  Â  Â  # response = requests.get(api_url, headers=headers, params=search_params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.warning("LinkedIn API integration not implemented. Add your API credentials.")

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ LinkedIn API failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  def _enhance_linkedin_profile_data(self, contact: ContactResult, profile_url: str) -> ContactResult:

Â  Â  Â  Â  """Enhance contact data by analyzing LinkedIn profile URL"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Extract LinkedIn username from URL

Â  Â  Â  Â  Â  Â  username_match = re.search(r'/in/([^/?]+)', profile_url)

Â  Â  Â  Â  Â  Â  if username_match:

Â  Â  Â  Â  Â  Â  Â  Â  username = username_match.group(1)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Add additional metadata

Â  Â  Â  Â  Â  Â  Â  Â  contact.linkedin_url = profile_url

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Generate potential email patterns based on name and common patterns

Â  Â  Â  Â  Â  Â  Â  Â  if contact.name and contact.company:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  potential_emails = self._generate_email_patterns(contact.name, contact.company)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # You could validate these emails separately

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact.summary = f"Potential emails: {', '.join(potential_emails[:3])}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return contact

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ LinkedIn profile enhancement failed: {e}")

Â  Â  Â  Â  Â  Â  return contact

Â  Â Â 

Â  Â  def _generate_email_patterns(self, name: str, company: str) -> List[str]:

Â  Â  Â  Â  """Generate common email patterns for a person/company"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not name or not company:

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Clean inputs

Â  Â  Â  Â  Â  Â  name_parts = name.lower().replace('.', '').split()

Â  Â  Â  Â  Â  Â  company_clean = re.sub(r'[^a-zA-Z0-9]', '', company.lower())

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if len(name_parts) < 2:

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  first_name = name_parts[0]

Â  Â  Â  Â  Â  Â  last_name = name_parts[-1]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Common email patterns

Â  Â  Â  Â  Â  Â  patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name}.{last_name}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{last_name}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name[0]}{last_name}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name}{last_name[0]}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name}_{last_name}@{company_clean}.com",

Â  Â  Â  Â  Â  Â  Â  Â  f"{first_name}{last_name}@{company_clean}.com"

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return patterns

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Email pattern generation failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def search_contacts(self, params: SearchParameters, enable_deduplication: bool = True) -> List[ContactResult]:

Â  Â  Â  Â  """Main method to search for contacts with optional deduplication"""

Â  Â  Â  Â  logger.info(f"ğŸ” Starting contact search with params: {params}")

Â  Â  Â  Â Â 

Â  Â  Â  Â  all_results = []

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Search across multiple sources

Â  Â  Â  Â  search_methods = [

Â  Â  Â  Â  Â  Â  self._search_google_contacts,

Â  Â  Â  Â  Â  Â  self._search_business_directories,

Â  Â  Â  Â  Â  Â  self._search_company_websites,

Â  Â  Â  Â  Â  Â  self._search_professional_networks

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  for method in search_methods:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  # Apply appropriate rate limiting based on method

Â  Â  Â  Â  Â  Â  Â  Â  if 'linkedin' in method.__name__.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await self._rate_limit_linkedin()

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await self._rate_limit()

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  results = await method(params)

Â  Â  Â  Â  Â  Â  Â  Â  all_results.extend(results)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Stop if we have enough results

Â  Â  Â  Â  Â  Â  Â  Â  if len(all_results) >= params.max_results:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"âš ï¸ Search method {method.__name__} failed: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Apply deduplication if enabled

Â  Â  Â  Â  if enable_deduplication:

Â  Â  Â  Â  Â  Â  unique_results, dedup_stats = await self.deduplicate_contacts(all_results)

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“Š Deduplication stats: {dedup_stats.get('duplicates_removed', 0)} duplicates removed")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  unique_results = all_results

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Sort by confidence and limit results

Â  Â  Â  Â  sorted_results = sorted(unique_results, key=lambda x: x.confidence_score, reverse=True)

Â  Â  Â  Â  final_results = sorted_results[:params.max_results]

Â  Â  Â  Â Â 

Â  Â  Â  Â  logger.info(f"âœ… Found {len(final_results)} unique contacts")

Â  Â  Â  Â  return final_results

Â  Â Â 

Â  Â  async def _search_google_contacts(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search for contacts using Google search"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Build Google search query

Â  Â  Â  Â  Â  Â  search_query = self._build_google_query(params)

Â  Â  Â  Â  Â  Â  google_url = f"https://www.google.com/search?q={quote(search_query)}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” Google searching: {search_query}")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  self.browser_page.get(google_url)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(3)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Parse Google results

Â  Â  Â  Â  Â  Â  search_results = self.browser_page.eles('css:.g')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for result in search_results[:10]:Â  # First 10 results

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact = self._parse_google_result(result, params)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if contact:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact.source = "Google Search"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append(contact)

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse Google result: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Google search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_business_directories(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search business directories"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Example directories (you can add more)

Â  Â  Â  Â  Â  Â  directories = [

Â  Â  Â  Â  Â  Â  Â  Â  "yellowpages.com",

Â  Â  Â  Â  Â  Â  Â  Â  "manta.com",

Â  Â  Â  Â  Â  Â  Â  Â  "crunchbase.com",

Â  Â  Â  Â  Â  Â  Â  Â  "apollo.io"

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for directory in directories:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await self._rate_limit()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  directory_results = await self._search_directory(directory, params)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.extend(directory_results)

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Directory {directory} failed: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Business directory search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_company_websites(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search company websites for contact information"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not params.company:

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Find company website

Â  Â  Â  Â  Â  Â  company_url = await self._find_company_website(params.company)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if company_url:

Â  Â  Â  Â  Â  Â  Â  Â  contacts = await self._scrape_company_contacts(company_url, params)

Â  Â  Â  Â  Â  Â  Â  Â  results.extend(contacts)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Company website search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_professional_networks(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search professional networks with LinkedIn support"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # LinkedIn search (with proper ToS considerations)

Â  Â  Â  Â  Â  Â  linkedin_results = await self._search_linkedin_carefully(params)

Â  Â  Â  Â  Â  Â  results.extend(linkedin_results)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Other professional networks can be added here

Â  Â  Â  Â  Â  Â  # AngelList, Xing, etc.

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Professional network search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_linkedin_carefully(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """

Â  Â  Â  Â  LinkedIn search with extreme caution and ToS compliance

Â  Â  Â  Â  WARNING: LinkedIn has strict ToS. Use responsibly and consider their API instead.

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # IMPORTANT: This should only be used for legitimate business purposes

Â  Â  Â  Â  Â  Â  # and in compliance with LinkedIn's Terms of Service

Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸ LinkedIn scraping - ensure ToS compliance!")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Build LinkedIn search URL (public search only)

Â  Â  Â  Â  Â  Â  search_query = self._build_linkedin_search_query(params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Use Google to find LinkedIn profiles (more ToS compliant)

Â  Â  Â  Â  Â  Â  google_linkedin_results = await self._search_linkedin_via_google(params)

Â  Â  Â  Â  Â  Â  results.extend(google_linkedin_results)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Direct LinkedIn search (use with extreme caution)

Â  Â  Â  Â  Â  Â  if len(results) < params.max_results // 2:Â  # Only if not enough results

Â  Â  Â  Â  Â  Â  Â  Â  direct_results = await self._search_linkedin_direct(params)

Â  Â  Â  Â  Â  Â  Â  Â  results.extend(direct_results)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ LinkedIn search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_linkedin_via_google(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search LinkedIn profiles via Google (more ToS compliant)"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Build Google search for LinkedIn profiles

Â  Â  Â  Â  Â  Â  search_terms = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if params.position:

Â  Â  Â  Â  Â  Â  Â  Â  search_terms.append(f'"{params.position}"')

Â  Â  Â  Â  Â  Â  if params.company:

Â  Â  Â  Â  Â  Â  Â  Â  search_terms.append(f'"{params.company}"')

Â  Â  Â  Â  Â  Â  if params.industry:

Â  Â  Â  Â  Â  Â  Â  Â  search_terms.append(f'"{params.industry}"')

Â  Â  Â  Â  Â  Â  if params.location or params.country:

Â  Â  Â  Â  Â  Â  Â  Â  location = f"{params.city or ''} {params.country or ''}".strip()

Â  Â  Â  Â  Â  Â  Â  Â  search_terms.append(f'"{location}"')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  search_query = " ".join(search_terms) + " site:linkedin.com/in/"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  google_url = f"https://www.google.com/search?q={quote(search_query)}&num=20"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” Searching LinkedIn via Google: {search_query}")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  await self._rate_limit_linkedin()

Â  Â  Â  Â  Â  Â  self.browser_page.get(google_url)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(3)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â  search_results = self.browser_page.eles('css:.g')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for result in search_results[:10]:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  link_elem = result.ele('css:a', timeout=1)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title_elem = result.ele('css:h3', timeout=1)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  snippet_elem = result.ele('css:.VwiC3b', timeout=1)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not link_elem or not title_elem:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  url = link_elem.attr('href')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title = title_elem.text or ""

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  snippet = snippet_elem.text if snippet_elem else ""

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Verify it's a LinkedIn profile URL

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'linkedin.com/in/' not in url:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Parse profile information from search result

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact = self._parse_linkedin_search_result(title, snippet, url, params)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if contact:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Enhance with additional LinkedIn data

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact = self._enhance_linkedin_profile_data(contact, url)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append(contact)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse LinkedIn search result: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found {len(results)} LinkedIn profiles via Google")

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ LinkedIn Google search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  async def _search_linkedin_direct(self, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Direct LinkedIn search - USE WITH EXTREME CAUTION

Â  Â  Â  Â  WARNING: This may violate LinkedIn ToS. Consider using their API instead.

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸ CAUTION: Direct LinkedIn search - ensure compliance!")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # This is a very basic implementation

Â  Â  Â  Â  Â  Â  # In production, you should use LinkedIn's official API

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Build LinkedIn search URL

Â  Â  Â  Â  Â  Â  search_params_dict = {}

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if params.position:

Â  Â  Â  Â  Â  Â  Â  Â  search_params_dict['keywords'] = params.position

Â  Â  Â  Â  Â  Â  if params.company:

Â  Â  Â  Â  Â  Â  Â  Â  search_params_dict['currentCompany'] = params.company

Â  Â  Â  Â  Â  Â  if params.location or params.country:

Â  Â  Â  Â  Â  Â  Â  Â  location = f"{params.city or ''} {params.country or ''}".strip()

Â  Â  Â  Â  Â  Â  Â  Â  search_params_dict['geoUrn'] = location

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Convert to URL parameters

Â  Â  Â  Â  Â  Â  url_params = "&".join([f"{k}={quote(str(v))}" for k, v in search_params_dict.items()])

Â  Â  Â  Â  Â  Â  linkedin_search_url = f"https://www.linkedin.com/search/results/people/?{url_params}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ” Direct LinkedIn search: {linkedin_search_url}")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Apply extra delay for LinkedIn

Â  Â  Â  Â  Â  Â  await asyncio.sleep(random.uniform(3, 6))

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  self.browser_page.get(linkedin_search_url)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(5)Â  # Wait for page load

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check if we're blocked or need to login

Â  Â  Â  Â  Â  Â  page_content = self.browser_page.html.lower()

Â  Â  Â  Â  Â  Â  if 'sign in' in page_content or 'join linkedin' in page_content:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("âš ï¸ LinkedIn requires authentication - switching to public search")

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Parse LinkedIn search results (this is a basic example)

Â  Â  Â  Â  Â  Â  profile_cards = self.browser_page.eles('css:.entity-result__item')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for card in profile_cards[:5]:Â  # Limit to prevent blocking

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await asyncio.sleep(random.uniform(1, 2))Â  # Rate limiting

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contact = await self._parse_linkedin_profile_card(card, params)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if contact:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append(contact)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse LinkedIn profile card: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Found {len(results)} LinkedIn profiles directly")

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Direct LinkedIn search failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  def _parse_linkedin_search_result(self, title: str, snippet: str, url: str, params: SearchParameters) -> Optional[ContactResult]:

Â  Â  Â  Â  """Parse LinkedIn profile from Google search result"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Extract name from title (usually "FirstName LastName - Position at Company")

Â  Â  Â  Â  Â  Â  name_match = re.match(r'^([^-|]+?)(?:\s*[-|]|$)', title)

Â  Â  Â  Â  Â  Â  name = name_match.group(1).strip() if name_match else "Unknown"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract position and company from title

Â  Â  Â  Â  Â  Â  position_company_match = re.search(r'-\s*(.+?)\s*(?:at|@)\s*(.+?)(?:\s*[-|]|$)', title)

Â  Â  Â  Â  Â  Â  if position_company_match:

Â  Â  Â  Â  Â  Â  Â  Â  position = position_company_match.group(1).strip()

Â  Â  Â  Â  Â  Â  Â  Â  company = position_company_match.group(2).strip()

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  # Try alternative patterns

Â  Â  Â  Â  Â  Â  Â  Â  position = self._extract_position_from_text(title + " " + snippet, params.position)

Â  Â  Â  Â  Â  Â  Â  Â  company = self._extract_company_from_text(title + " " + snippet, params.company)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract location from snippet

Â  Â  Â  Â  Â  Â  location = self._extract_location_from_text(snippet, params.country, params.city)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Calculate confidence score

Â  Â  Â  Â  Â  Â  confidence = self._calculate_linkedin_confidence(name, position, company, params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if confidence > 0.4:Â  # Higher threshold for LinkedIn

Â  Â  Â  Â  Â  Â  Â  Â  return ContactResult(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name=name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  position=position,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company=company,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  location=location,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linkedin_url=url,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  profile_url=url,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source="LinkedIn (via Google)",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_score=confidence,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary=snippet[:200] if snippet else None

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse LinkedIn search result: {e}")

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  async def _parse_linkedin_profile_card(self, card_element, params: SearchParameters) -> Optional[ContactResult]:

Â  Â  Â  Â  """Parse LinkedIn profile card element"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Extract name

Â  Â  Â  Â  Â  Â  name_elem = card_element.ele('css:.entity-result__title-text a span[aria-hidden="true"]', timeout=2)

Â  Â  Â  Â  Â  Â  name = name_elem.text.strip() if name_elem and name_elem.text else "Unknown"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract profile URL

Â  Â  Â  Â  Â  Â  profile_link_elem = card_element.ele('css:.entity-result__title-text a', timeout=2)

Â  Â  Â  Â  Â  Â  profile_url = profile_link_elem.attr('href') if profile_link_elem else ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract position

Â  Â  Â  Â  Â  Â  position_elem = card_element.ele('css:.entity-result__primary-subtitle', timeout=2)

Â  Â  Â  Â  Â  Â  position = position_elem.text.strip() if position_elem and position_elem.text else None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract company and location

Â  Â  Â  Â  Â  Â  secondary_elem = card_element.ele('css:.entity-result__secondary-subtitle', timeout=2)

Â  Â  Â  Â  Â  Â  secondary_text = secondary_elem.text if secondary_elem and secondary_elem.text else ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Parse company and location from secondary text

Â  Â  Â  Â  Â  Â  company = None

Â  Â  Â  Â  Â  Â  location = None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if secondary_text:

Â  Â  Â  Â  Â  Â  Â  Â  # Usually in format "Company â€¢ Location" or just "Company"

Â  Â  Â  Â  Â  Â  Â  Â  parts = secondary_text.split('â€¢')

Â  Â  Â  Â  Â  Â  Â  Â  if len(parts) >= 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company = parts[0].strip()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  location = parts[1].strip()

Â  Â  Â  Â  Â  Â  Â  Â  elif len(parts) == 1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Could be company or location

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if any(keyword in parts[0].lower() for keyword in ['inc', 'corp', 'ltd', 'llc', 'company']):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company = parts[0].strip()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  location = parts[0].strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Calculate confidence

Â  Â  Â  Â  Â  Â  confidence = self._calculate_linkedin_confidence(name, position, company, params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if confidence > 0.4:

Â  Â  Â  Â  Â  Â  Â  Â  return ContactResult(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name=name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  position=position,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company=company,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  location=location,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linkedin_url=profile_url,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  profile_url=profile_url,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source="LinkedIn (Direct)",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_score=confidence

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse LinkedIn profile card: {e}")

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _calculate_linkedin_confidence(self, name: Optional[str], position: Optional[str],Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â company: Optional[str], params: SearchParameters) -> float:

Â  Â  Â  Â  """Calculate confidence score for LinkedIn profiles"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  score = 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Base score for having a name

Â  Â  Â  Â  Â  Â  if name and name != "Unknown":

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.4Â  # Higher base for LinkedIn

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Position match (higher weight for LinkedIn)

Â  Â  Â  Â  Â  Â  if position and params.position:

Â  Â  Â  Â  Â  Â  Â  Â  if params.position.lower() in position.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.4

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â  elif position:

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Company match (higher weight)

Â  Â  Â  Â  Â  Â  if company and params.company:

Â  Â  Â  Â  Â  Â  Â  Â  if params.company.lower() in company.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.3

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â  elif company:

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # LinkedIn profiles are generally more reliable

Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return min(score, 1.0)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _build_linkedin_search_query(self, params: SearchParameters) -> str:

Â  Â  Â  Â  """Build LinkedIn search query"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  parts = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if params.position:

Â  Â  Â  Â  Â  Â  Â  Â  parts.append(f'title:"{params.position}"')

Â  Â  Â  Â  Â  Â  if params.company:

Â  Â  Â  Â  Â  Â  Â  Â  parts.append(f'company:"{params.company}"')

Â  Â  Â  Â  Â  Â  if params.industry:

Â  Â  Â  Â  Â  Â  Â  Â  parts.append(f'industry:"{params.industry}"')

Â  Â  Â  Â  Â  Â  if params.location or params.country:

Â  Â  Â  Â  Â  Â  Â  Â  location = f"{params.city or ''} {params.country or ''}".strip()

Â  Â  Â  Â  Â  Â  Â  Â  parts.append(f'location:"{location}"')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return " AND ".join(parts) if parts else ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return ""

Â  Â Â 

Â  Â  def _build_google_query(self, params: SearchParameters) -> str:

Â  Â  Â  Â  """Build optimized Google search query"""

Â  Â  Â  Â  query_parts = []

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Core search terms

Â  Â  Â  Â  if params.position and params.company:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.position}" "{params.company}"')

Â  Â  Â  Â  elif params.position:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.position}"')

Â  Â  Â  Â  elif params.company:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.company}"')

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Location

Â  Â  Â  Â  if params.city and params.country:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.city}, {params.country}"')

Â  Â  Â  Â  elif params.country:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.country}"')

Â  Â  Â  Â  elif params.city:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.city}"')

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Industry

Â  Â  Â  Â  if params.industry:

Â  Â  Â  Â  Â  Â  query_parts.append(f'"{params.industry}"')

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Add contact-specific terms

Â  Â  Â  Â  query_parts.append('(email OR contact OR linkedin OR profile)')

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Exclude job boards and generic sites

Â  Â  Â  Â  query_parts.append('-indeed.com -glassdoor.com -jobsite.com')

Â  Â  Â  Â Â 

Â  Â  Â  Â  return " ".join(query_parts)

Â  Â Â 

Â  Â  def _parse_google_result(self, result_element, params: SearchParameters) -> Optional[ContactResult]:

Â  Â  Â  Â  """Parse individual Google search result"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Extract title, link, and snippet

Â  Â  Â  Â  Â  Â  title_elem = result_element.ele('css:h3', timeout=1)

Â  Â  Â  Â  Â  Â  link_elem = result_element.ele('css:a', timeout=1)

Â  Â  Â  Â  Â  Â  snippet_elem = result_element.ele('css:.VwiC3b', timeout=1)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not title_elem or not link_elem:

Â  Â  Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  title = title_elem.text or ""

Â  Â  Â  Â  Â  Â  url = link_elem.attr('href') or ""

Â  Â  Â  Â  Â  Â  snippet = snippet_elem.text if snippet_elem else ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract information from title and snippet

Â  Â  Â  Â  Â  Â  name = self._extract_name_from_text(title)

Â  Â  Â  Â  Â  Â  position = self._extract_position_from_text(title + " " + snippet, params.position)

Â  Â  Â  Â  Â  Â  company = self._extract_company_from_text(title + " " + snippet, params.company)

Â  Â  Â  Â  Â  Â  location = self._extract_location_from_text(snippet, params.country, params.city)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Try to find email pattern

Â  Â  Â  Â  Â  Â  email = self._extract_email_from_text(snippet)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Calculate confidence score

Â  Â  Â  Â  Â  Â  confidence = self._calculate_confidence(name, position, company, params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if confidence > 0.3:Â  # Minimum confidence threshold

Â  Â  Â  Â  Â  Â  Â  Â  return ContactResult(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name=name or "Unknown",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  position=position,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company=company,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  location=location,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  email=email,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  profile_url=url,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_score=confidence,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  summary=snippet[:200] if snippet else None

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to parse Google result: {e}")

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  async def _search_directory(self, directory: str, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Search specific business directory"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # This is a template - implement specific directory parsing

Â  Â  Â  Â  Â  Â  logger.info(f"Searching {directory}")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Build directory-specific search URL

Â  Â  Â  Â  Â  Â  search_url = self._build_directory_url(directory, params)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not search_url:

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  self.browser_page.get(search_url)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(3)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Directory-specific parsing would go here

Â  Â  Â  Â  Â  Â  # This is a placeholder

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Directory search failed for {directory}: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  def _build_directory_url(self, directory: str, params: SearchParameters) -> str:

Â  Â  Â  Â  """Build directory-specific search URL"""

Â  Â  Â  Â  # Implement directory-specific URL building

Â  Â  Â  Â  base_urls = {

Â  Â  Â  Â  Â  Â  "yellowpages.com": "https://www.yellowpages.com/search?search_terms={}&geo_location_terms={}",

Â  Â  Â  Â  Â  Â  "manta.com": "https://www.manta.com/search?search={}&location={}",

Â  Â  Â  Â  Â  Â  "crunchbase.com": "https://www.crunchbase.com/discover/organization.companies/{}",

Â  Â  Â  Â  Â  Â  "apollo.io": "https://app.apollo.io/#/people?{}"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  if directory in base_urls:

Â  Â  Â  Â  Â  Â  search_term = params.to_search_string()

Â  Â  Â  Â  Â  Â  location = f"{params.city or ''} {params.country or ''}".strip()

Â  Â  Â  Â  Â  Â  return base_urls[directory].format(quote(search_term), quote(location))

Â  Â  Â  Â Â 

Â  Â  Â  Â  return ""

Â  Â Â 

Â  Â  async def _find_company_website(self, company_name: str) -> Optional[str]:

Â  Â  Â  Â  """Find company website URL"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Search for company website

Â  Â  Â  Â  Â  Â  search_query = f'"{company_name}" site:official OR site:company OR website'

Â  Â  Â  Â  Â  Â  google_url = f"https://www.google.com/search?q={quote(search_query)}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  self.browser_page.get(google_url)

Â  Â  Â  Â  Â  Â  await asyncio.sleep(2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Get first result that looks like company website

Â  Â  Â  Â  Â  Â  results = self.browser_page.eles('css:.g a')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for result in results[:5]:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  url = result.attr('href')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if url and self._is_company_website(url, company_name):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return url

Â  Â  Â  Â  Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Company website search failed: {e}")

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _is_company_website(self, url: str, company_name: str) -> bool:

Â  Â  Â  Â  """Check if URL is likely a company website"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  domain = urlparse(url).netloc.lower()

Â  Â  Â  Â  Â  Â  company_clean = re.sub(r'[^a-zA-Z0-9]', '', company_name.lower())

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check if company name is in domain

Â  Â  Â  Â  Â  Â  if company_clean in domain.replace('.', '').replace('-', ''):

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Exclude common non-company domains

Â  Â  Â  Â  Â  Â  excluded_domains = [

Â  Â  Â  Â  Â  Â  Â  Â  'linkedin.com', 'facebook.com', 'twitter.com', 'instagram.com',

Â  Â  Â  Â  Â  Â  Â  Â  'wikipedia.org', 'crunchbase.com', 'bloomberg.com', 'reuters.com',

Â  Â  Â  Â  Â  Â  Â  Â  'google.com', 'youtube.com', 'indeed.com', 'glassdoor.com'

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for excluded in excluded_domains:

Â  Â  Â  Â  Â  Â  Â  Â  if excluded in domain:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return False

Â  Â Â 

Â  Â  async def _scrape_company_contacts(self, company_url: str, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Scrape contacts from company website"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  await self._create_browser()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Common contact pages

Â  Â  Â  Â  Â  Â  contact_pages = [

Â  Â  Â  Â  Â  Â  Â  Â  company_url,

Â  Â  Â  Â  Â  Â  Â  Â  urljoin(company_url, '/about'),

Â  Â  Â  Â  Â  Â  Â  Â  urljoin(company_url, '/team'),

Â  Â  Â  Â  Â  Â  Â  Â  urljoin(company_url, '/contact'),

Â  Â  Â  Â  Â  Â  Â  Â  urljoin(company_url, '/leadership'),

Â  Â  Â  Â  Â  Â  Â  Â  urljoin(company_url, '/management')

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for page_url in contact_pages:

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await self._rate_limit()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.browser_page.get(page_url)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await asyncio.sleep(2)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  page_contacts = self._extract_contacts_from_page(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.browser_page.html,Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company_url,Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  params

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.extend(page_contacts)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Failed to scrape {page_url}: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Company contact scraping failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  def _extract_contacts_from_page(self, html: str, base_url: str, params: SearchParameters) -> List[ContactResult]:

Â  Â  Â  Â  """Extract contacts from HTML page"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  contacts = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Find email addresses

Â  Â  Â  Â  Â  Â  emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', html)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Find phone numbers

Â  Â  Â  Â  Â  Â  phones = re.findall(r'(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', html)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Find potential names and positions (basic implementation)

Â  Â  Â  Â  Â  Â  # This would need more sophisticated NLP for better results

Â  Â  Â  Â  Â  Â  name_position_patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  r'<h[1-6][^>]*>([^<]+)</h[1-6]>',

Â  Â  Â  Â  Â  Â  Â  Â  r'<p[^>]*><strong>([^<]+)</strong>',

Â  Â  Â  Â  Â  Â  Â  Â  r'<div[^>]*class="[^"]*name[^"]*"[^>]*>([^<]+)</div>',

Â  Â  Â  Â  Â  Â  Â  Â  r'<span[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</span>'

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  potential_contacts = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for pattern in name_position_patterns:

Â  Â  Â  Â  Â  Â  Â  Â  matches = re.findall(pattern, html, re.IGNORECASE)

Â  Â  Â  Â  Â  Â  Â  Â  potential_contacts.extend(matches)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Process found information

Â  Â  Â  Â  Â  Â  for i, email in enumerate(emails[:5]):Â  # Limit to first 5 emails

Â  Â  Â  Â  Â  Â  Â  Â  name = self._extract_name_from_email(email)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  contact = ContactResult(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name=name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  email=email,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company=params.company,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source=f"Company Website: {base_url}",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  confidence_score=0.7Â  # Medium confidence for website contacts

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  contacts.append(contact)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return contacts

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Contact extraction failed: {e}")

Â  Â  Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  # Utility methods for text extraction

Â  Â  def _extract_name_from_text(self, text: str) -> Optional[str]:

Â  Â  Â  Â  """Extract person name from text"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Simple name extraction (can be improved with NLP)

Â  Â  Â  Â  Â  Â  name_patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-z]+ [A-Z][a-z]+)\b',Â  # First Last

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-z]+ [A-Z]\. [A-Z][a-z]+)\b',Â  # First M. Last

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+)\b'Â  # First Middle Last

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for pattern in name_patterns:

Â  Â  Â  Â  Â  Â  Â  Â  matches = re.findall(pattern, text)

Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return matches[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _extract_position_from_text(self, text: str, target_position: Optional[str] = None) -> Optional[str]:

Â  Â  Â  Â  """Extract job position from text"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Common position keywords

Â  Â  Â  Â  Â  Â  position_patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  r'\b(CEO|CTO|CFO|COO|VP|Director|Manager|Lead|Senior|Principal|Head of)\b[^,\.]{0,50}',

Â  Â  Â  Â  Â  Â  Â  Â  r'\b(President|Founder|Co-founder|Partner|Executive|Analyst|Specialist|Engineer|Developer)\b[^,\.]{0,30}'

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for pattern in position_patterns:

Â  Â  Â  Â  Â  Â  Â  Â  matches = re.findall(pattern, text, re.IGNORECASE)

Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return matches[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # If target position provided, look for it specifically

Â  Â  Â  Â  Â  Â  if target_position:

Â  Â  Â  Â  Â  Â  Â  Â  pattern = rf'\b{re.escape(target_position)}\b'

Â  Â  Â  Â  Â  Â  Â  Â  if re.search(pattern, text, re.IGNORECASE):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return target_position

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _extract_company_from_text(self, text: str, target_company: Optional[str] = None) -> Optional[str]:

Â  Â  Â  Â  """Extract company name from text"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # If target company provided, look for it

Â  Â  Â  Â  Â  Â  if target_company:

Â  Â  Â  Â  Â  Â  Â  Â  pattern = rf'\b{re.escape(target_company)}\b'

Â  Â  Â  Â  Â  Â  Â  Â  if re.search(pattern, text, re.IGNORECASE):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return target_company

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Look for company indicators

Â  Â  Â  Â  Â  Â  company_patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  r'\bat\s+([A-Z][a-zA-Z\s&]+(?:Inc|LLC|Corp|Ltd|Co)\.?)\b',

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-zA-Z\s&]+(?:Inc|LLC|Corp|Ltd|Co)\.?)\b',

Â  Â  Â  Â  Â  Â  Â  Â  r'\bworks?\s+at\s+([A-Z][a-zA-Z\s&]+)\b'

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for pattern in company_patterns:

Â  Â  Â  Â  Â  Â  Â  Â  matches = re.findall(pattern, text, re.IGNORECASE)

Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return matches[0].strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _extract_location_from_text(self, text: str, target_country: Optional[str] = None, target_city: Optional[str] = None) -> Optional[str]:

Â  Â  Â  Â  """Extract location from text"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Check for target location first

Â  Â  Â  Â  Â  Â  if target_city and target_country:

Â  Â  Â  Â  Â  Â  Â  Â  pattern = rf'\b{re.escape(target_city)}[,\s]+{re.escape(target_country)}\b'

Â  Â  Â  Â  Â  Â  Â  Â  if re.search(pattern, text, re.IGNORECASE):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return f"{target_city}, {target_country}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Common location patterns

Â  Â  Â  Â  Â  Â  location_patterns = [

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-z]+,\s*[A-Z][a-z]+)\b',Â  # City, Country

Â  Â  Â  Â  Â  Â  Â  Â  r'\b([A-Z][a-z]+,\s*[A-Z]{2})\b',Â  Â  Â # City, State

Â  Â  Â  Â  Â  Â  Â  Â  r'\bin\s+([A-Z][a-z]+(?:,\s*[A-Z][a-z]+)?)\b'Â  # in Location

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for pattern in location_patterns:

Â  Â  Â  Â  Â  Â  Â  Â  matches = re.findall(pattern, text)

Â  Â  Â  Â  Â  Â  Â  Â  if matches:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return matches[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _extract_email_from_text(self, text: str) -> Optional[str]:

Â  Â  Â  Â  """Extract email from text"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

Â  Â  Â  Â  Â  Â  matches = re.findall(email_pattern, text)

Â  Â  Â  Â  Â  Â  return matches[0] if matches else None

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return None

Â  Â Â 

Â  Â  def _extract_name_from_email(self, email: str) -> str:

Â  Â  Â  Â  """Extract name from email address"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  username = email.split('@')[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Handle common email patterns

Â  Â  Â  Â  Â  Â  if '.' in username:

Â  Â  Â  Â  Â  Â  Â  Â  parts = username.split('.')

Â  Â  Â  Â  Â  Â  Â  Â  name_parts = [part.capitalize() for part in parts if len(part) > 1]

Â  Â  Â  Â  Â  Â  Â  Â  return ' '.join(name_parts)

Â  Â  Â  Â  Â  Â  elif '_' in username:

Â  Â  Â  Â  Â  Â  Â  Â  parts = username.split('_')

Â  Â  Â  Â  Â  Â  Â  Â  name_parts = [part.capitalize() for part in parts if len(part) > 1]

Â  Â  Â  Â  Â  Â  Â  Â  return ' '.join(name_parts)

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  return username.capitalize()

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return "Unknown"

Â  Â Â 

Â  Â  def _calculate_confidence(self, name: Optional[str], position: Optional[str],Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  company: Optional[str], params: SearchParameters) -> float:

Â  Â  Â  Â  """Calculate confidence score for a contact"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  score = 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Base score for having a name

Â  Â  Â  Â  Â  Â  if name and name != "Unknown":

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.3

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Position match

Â  Â  Â  Â  Â  Â  if position and params.position:

Â  Â  Â  Â  Â  Â  Â  Â  if params.position.lower() in position.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.3

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â  elif position:

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Company match

Â  Â  Â  Â  Â  Â  if company and params.company:

Â  Â  Â  Â  Â  Â  Â  Â  if params.company.lower() in company.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.3

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â  elif company:

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Additional factors

Â  Â  Â  Â  Â  Â  if name and len(name.split()) >= 2:Â  # Full name

Â  Â  Â  Â  Â  Â  Â  Â  score += 0.1

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return min(score, 1.0)Â  # Cap at 1.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _deduplicate_results(self, results: List[ContactResult]) -> List[ContactResult]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Advanced deduplication with fuzzy matching and data merging

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not results:

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ”„ Starting deduplication for {len(results)} contacts...")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Step 1: Group potentially duplicate contacts

Â  Â  Â  Â  Â  Â  duplicate_groups = self._group_duplicates(results)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Step 2: Merge duplicates within each group

Â  Â  Â  Â  Â  Â  deduplicated_contacts = []

Â  Â  Â  Â  Â  Â  for group in duplicate_groups:

Â  Â  Â  Â  Â  Â  Â  Â  if len(group) == 1:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  deduplicated_contacts.append(group[0])

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Merge multiple contacts into one

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  merged_contact = self._merge_contacts(group)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  deduplicated_contacts.append(merged_contact)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Step 3: Final validation and cleanup

Â  Â  Â  Â  Â  Â  final_contacts = self._final_dedup_validation(deduplicated_contacts)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  removed_count = len(results) - len(final_contacts)

Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Deduplication complete: {removed_count} duplicates removed, {len(final_contacts)} unique contacts")

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return final_contacts

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Deduplication failed: {e}")

Â  Â  Â  Â  Â  Â  # Fallback to basic deduplication

Â  Â  Â  Â  Â  Â  return self._basic_deduplicate(results)

Â  Â Â 

Â  Â  def _group_duplicates(self, results: List[ContactResult]) -> List[List[ContactResult]]:

Â  Â  Â  Â  """Group contacts that are likely duplicates"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  groups = []

Â  Â  Â  Â  Â  Â  processed = set()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for i, contact in enumerate(results):

Â  Â  Â  Â  Â  Â  Â  Â  if i in processed:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Start a new group with this contact

Â  Â  Â  Â  Â  Â  Â  Â  current_group = [contact]

Â  Â  Â  Â  Â  Â  Â  Â  processed.add(i)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Find all other contacts that match this one

Â  Â  Â  Â  Â  Â  Â  Â  for j, other_contact in enumerate(results[i+1:], i+1):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if j in processed:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self._are_duplicates(contact, other_contact):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_group.append(other_contact)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processed.add(j)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  groups.append(current_group)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return groups

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Grouping duplicates failed: {e}")

Â  Â  Â  Â  Â  Â  return [[contact] for contact in results]

Â  Â Â 

Â  Â  def _are_duplicates(self, contact1: ContactResult, contact2: ContactResult) -> bool:

Â  Â  Â  Â  """

Â  Â  Â  Â  Determine if two contacts are duplicates using multiple criteria

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Exact email match (highest priority)

Â  Â  Â  Â  Â  Â  if contact1.email and contact2.email:

Â  Â  Â  Â  Â  Â  Â  Â  if contact1.email.lower() == contact2.email.lower():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Exact LinkedIn URL match

Â  Â  Â  Â  Â  Â  if contact1.linkedin_url and contact2.linkedin_url:

Â  Â  Â  Â  Â  Â  Â  Â  if self._normalize_linkedin_url(contact1.linkedin_url) == self._normalize_linkedin_url(contact2.linkedin_url):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Exact phone match

Â  Â  Â  Â  Â  Â  if contact1.phone and contact2.phone:

Â  Â  Â  Â  Â  Â  Â  Â  if self._normalize_phone(contact1.phone) == self._normalize_phone(contact2.phone):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Name + Company similarity

Â  Â  Â  Â  Â  Â  name_similarity = self._calculate_name_similarity(contact1.name, contact2.name)

Â  Â  Â  Â  Â  Â  company_similarity = self._calculate_company_similarity(contact1.company, contact2.company)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # High name similarity + same company = duplicate

Â  Â  Â  Â  Â  Â  if (name_similarity >= self.dedup_thresholds["name_similarity"] andÂ 

Â  Â  Â  Â  Â  Â  Â  Â  company_similarity >= self.dedup_thresholds["company_similarity"]):

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Very high name similarity + similar company = duplicate

Â  Â  Â  Â  Â  Â  if (name_similarity >= 0.95 andÂ 

Â  Â  Â  Â  Â  Â  Â  Â  company_similarity >= self.dedup_thresholds["company_similarity"] * 0.8):

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Same email domain + high name similarity = likely duplicate

Â  Â  Â  Â  Â  Â  if contact1.email and contact2.email:

Â  Â  Â  Â  Â  Â  Â  Â  domain1 = contact1.email.split('@')[1].lower() if '@' in contact1.email else ''

Â  Â  Â  Â  Â  Â  Â  Â  domain2 = contact2.email.split('@')[1].lower() if '@' in contact2.email else ''

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if (domain1 == domain2 and domain1 andÂ 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  name_similarity >= self.dedup_thresholds["email_domain_name_threshold"]):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Same name + similar position + similar location = duplicate

Â  Â  Â  Â  Â  Â  if (name_similarity >= self.dedup_thresholds["name_similarity"] andÂ 

Â  Â  Â  Â  Â  Â  Â  Â  self._calculate_position_similarity(contact1.position, contact2.position) >= self.dedup_thresholds["position_similarity"] and

Â  Â  Â  Â  Â  Â  Â  Â  self._calculate_location_similarity(contact1.location, contact2.location) >= self.dedup_thresholds["location_similarity"]):

Â  Â  Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return False

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Duplicate check failed: {e}")

Â  Â  Â  Â  Â  Â  return False

Â  Â Â 

Â  Â  def _calculate_name_similarity(self, name1: Optional[str], name2: Optional[str]) -> float:

Â  Â  Â  Â  """Calculate similarity between two names"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not name1 or not name2:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Normalize names

Â  Â  Â  Â  Â  Â  name1_clean = self._normalize_name(name1)

Â  Â  Â  Â  Â  Â  name2_clean = self._normalize_name(name2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not name1_clean or not name2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Exact match

Â  Â  Â  Â  Â  Â  if name1_clean == name2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 1.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Split into parts and compare

Â  Â  Â  Â  Â  Â  parts1 = name1_clean.split()

Â  Â  Â  Â  Â  Â  parts2 = name2_clean.split()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check if one name is contained in the other

Â  Â  Â  Â  Â  Â  if len(parts1) >= 2 and len(parts2) >= 2:

Â  Â  Â  Â  Â  Â  Â  Â  # Compare first and last names

Â  Â  Â  Â  Â  Â  Â  Â  first_similarity = SequenceMatcher(None, parts1[0], parts2[0]).ratio()

Â  Â  Â  Â  Â  Â  Â  Â  last_similarity = SequenceMatcher(None, parts1[-1], parts2[-1]).ratio()

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # If first and last names are very similar, consider it a match

Â  Â  Â  Â  Â  Â  Â  Â  if first_similarity >= 0.9 and last_similarity >= 0.9:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 0.95

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Check for initials match (e.g., "John Smith" vs "J. Smith")

Â  Â  Â  Â  Â  Â  Â  Â  if (len(parts1[0]) == 1 and parts1[0].lower() == parts2[0][0].lower() and last_similarity >= 0.9):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 0.85

Â  Â  Â  Â  Â  Â  Â  Â  if (len(parts2[0]) == 1 and parts2[0].lower() == parts1[0][0].lower() and last_similarity >= 0.9):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 0.85

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Overall string similarity

Â  Â  Â  Â  Â  Â  return SequenceMatcher(None, name1_clean, name2_clean).ratio()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Name similarity calculation failed: {e}")

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _calculate_company_similarity(self, company1: Optional[str], company2: Optional[str]) -> float:

Â  Â  Â  Â  """Calculate similarity between two companies"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not company1 or not company2:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Normalize company names

Â  Â  Â  Â  Â  Â  comp1_clean = self._normalize_company_name(company1)

Â  Â  Â  Â  Â  Â  comp2_clean = self._normalize_company_name(company2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not comp1_clean or not comp2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Exact match

Â  Â  Â  Â  Â  Â  if comp1_clean == comp2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 1.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check if one is contained in the other

Â  Â  Â  Â  Â  Â  if comp1_clean in comp2_clean or comp2_clean in comp1_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.9

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # String similarity

Â  Â  Â  Â  Â  Â  return SequenceMatcher(None, comp1_clean, comp2_clean).ratio()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Company similarity calculation failed: {e}")

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _calculate_position_similarity(self, pos1: Optional[str], pos2: Optional[str]) -> float:

Â  Â  Â  Â  """Calculate similarity between two positions"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not pos1 or not pos2:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  pos1_clean = self._normalize_position(pos1)

Â  Â  Â  Â  Â  Â  pos2_clean = self._normalize_position(pos2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not pos1_clean or not pos2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if pos1_clean == pos2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 1.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check for common title patterns

Â  Â  Â  Â  Â  Â  title_synonyms = {

Â  Â  Â  Â  Â  Â  Â  Â  'engineer': ['developer', 'programmer', 'coder'],

Â  Â  Â  Â  Â  Â  Â  Â  'manager': ['lead', 'supervisor', 'director'],

Â  Â  Â  Â  Â  Â  Â  Â  'analyst': ['specialist', 'consultant'],

Â  Â  Â  Â  Â  Â  Â  Â  'executive': ['director', 'vp', 'president']

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for key, synonyms in title_synonyms.items():

Â  Â  Â  Â  Â  Â  Â  Â  if key in pos1_clean and any(syn in pos2_clean for syn in synonyms):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 0.8

Â  Â  Â  Â  Â  Â  Â  Â  if key in pos2_clean and any(syn in pos1_clean for syn in synonyms):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return 0.8

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return SequenceMatcher(None, pos1_clean, pos2_clean).ratio()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Position similarity calculation failed: {e}")

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _calculate_location_similarity(self, loc1: Optional[str], loc2: Optional[str]) -> float:

Â  Â  Â  Â  """Calculate similarity between two locations"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not loc1 or not loc2:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  loc1_clean = self._normalize_location(loc1)

Â  Â  Â  Â  Â  Â  loc2_clean = self._normalize_location(loc2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not loc1_clean or not loc2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if loc1_clean == loc2_clean:

Â  Â  Â  Â  Â  Â  Â  Â  return 1.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Check if locations share city or country

Â  Â  Â  Â  Â  Â  parts1 = loc1_clean.split(',')

Â  Â  Â  Â  Â  Â  parts2 = loc2_clean.split(',')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Compare individual parts

Â  Â  Â  Â  Â  Â  max_similarity = 0.0

Â  Â  Â  Â  Â  Â  for part1 in parts1:

Â  Â  Â  Â  Â  Â  Â  Â  for part2 in parts2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  similarity = SequenceMatcher(None, part1.strip(), part2.strip()).ratio()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_similarity = max(max_similarity, similarity)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return max_similarity

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Location similarity calculation failed: {e}")

Â  Â  Â  Â  Â  Â  return 0.0

Â  Â Â 

Â  Â  def _merge_contacts(self, contacts: List[ContactResult]) -> ContactResult:

Â  Â  Â  Â  """

Â  Â  Â  Â  Merge multiple contact records into a single, comprehensive record

Â  Â  Â  Â  """

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if len(contacts) == 1:

Â  Â  Â  Â  Â  Â  Â  Â  return contacts[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Choose the contact with highest confidence as base

Â  Â  Â  Â  Â  Â  base_contact = max(contacts, key=lambda c: c.confidence_score)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Merge data from all contacts

Â  Â  Â  Â  Â  Â  merged_data = {

Â  Â  Â  Â  Â  Â  Â  Â  'name': self._merge_field([c.name for c in contacts], 'name'),

Â  Â  Â  Â  Â  Â  Â  Â  'position': self._merge_field([c.position for c in contacts], 'position'),

Â  Â  Â  Â  Â  Â  Â  Â  'company': self._merge_field([c.company for c in contacts], 'company'),

Â  Â  Â  Â  Â  Â  Â  Â  'location': self._merge_field([c.location for c in contacts], 'location'),

Â  Â  Â  Â  Â  Â  Â  Â  'email': self._merge_field([c.email for c in contacts], 'email'),

Â  Â  Â  Â  Â  Â  Â  Â  'phone': self._merge_field([c.phone for c in contacts], 'phone'),

Â  Â  Â  Â  Â  Â  Â  Â  'linkedin_url': self._merge_field([c.linkedin_url for c in contacts], 'linkedin_url'),

Â  Â  Â  Â  Â  Â  Â  Â  'profile_url': self._merge_field([c.profile_url for c in contacts], 'profile_url'),

Â  Â  Â  Â  Â  Â  Â  Â  'industry': self._merge_field([c.industry for c in contacts], 'industry'),

Â  Â  Â  Â  Â  Â  Â  Â  'experience': self._merge_field([c.experience for c in contacts], 'experience'),

Â  Â  Â  Â  Â  Â  Â  Â  'summary': self._merge_summaries([c.summary for c in contacts]),

Â  Â  Â  Â  Â  Â  Â  Â  'source': self._merge_sources([c.source for c in contacts]),

Â  Â  Â  Â  Â  Â  Â  Â  'scraped_at': base_contact.scraped_at,

Â  Â  Â  Â  Â  Â  Â  Â  'confidence_score': self._calculate_merged_confidence(contacts)

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Create merged contact

Â  Â  Â  Â  Â  Â  merged_contact = ContactResult(**merged_data)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  logger.debug(f"âœ… Merged {len(contacts)} contacts for {merged_contact.name}")

Â  Â  Â  Â  Â  Â  return merged_contact

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Contact merging failed: {e}")

Â  Â  Â  Â  Â  Â  # Return the contact with highest confidence

Â  Â  Â  Â  Â  Â  return max(contacts, key=lambda c: c.confidence_score)

Â  Â Â 

Â  Â  def _merge_field(self, values: List[Optional[str]], field_type: str) -> Optional[str]:

Â  Â  Â  Â  """Merge field values using field-specific logic"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Remove None and empty values

Â  Â  Â  Â  Â  Â  valid_values = [v for v in values if v and v.strip()]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if not valid_values:

Â  Â  Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  if len(valid_values) == 1:

Â  Â  Â  Â  Â  Â  Â  Â  return valid_values[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Field-specific merging logic

Â  Â  Â  Â  Â  Â  if field_type == 'name':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_names(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type == 'email':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_emails(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type == 'phone':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_phones(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type in ['linkedin_url', 'profile_url']:

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_urls(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type == 'position':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_positions(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type == 'company':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_companies(valid_values)

Â  Â  Â  Â  Â  Â  elif field_type == 'location':

Â  Â  Â  Â  Â  Â  Â  Â  return self._merge_locations(valid_values)

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  # Default: return the longest/most complete value

Â  Â  Â  Â  Â  Â  Â  Â  return max(valid_values, key=len)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Field merge failed for {field_type}: {e}")

Â  Â  Â  Â  Â  Â  return valid_values[0] if valid_values else None

Â  Â Â 

Â  Â  def _merge_names(self, names: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple name variations"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer the most complete name (most words)

Â  Â  Â  Â  Â  Â  return max(names, key=lambda n: len(n.split()))

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return names[0]

Â  Â Â 

Â  Â  def _merge_emails(self, emails: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple emails"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer business emails over personal ones

Â  Â  Â  Â  Â  Â  business_domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com']

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  business_emails = [e for e in emails if not any(domain in e.lower() for domain in business_domains)]

Â  Â  Â  Â  Â  Â  if business_emails:

Â  Â  Â  Â  Â  Â  Â  Â  return business_emails[0]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return emails[0]

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return emails[0]

Â  Â Â 

Â  Â  def _merge_phones(self, phones: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple phone numbers"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer the most complete phone number

Â  Â  Â  Â  Â  Â  return max(phones, key=lambda p: len(re.sub(r'[^\d]', '', p)))

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return phones[0]

Â  Â Â 

Â  Â  def _merge_urls(self, urls: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple URLs"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer LinkedIn URLs, then others

Â  Â  Â  Â  Â  Â  linkedin_urls = [u for u in urls if 'linkedin.com' in u.lower()]

Â  Â  Â  Â  Â  Â  if linkedin_urls:

Â  Â  Â  Â  Â  Â  Â  Â  return linkedin_urls[0]

Â  Â  Â  Â  Â  Â  return urls[0]

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return urls[0]

Â  Â Â 

Â  Â  def _merge_positions(self, positions: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple position titles"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer the most descriptive/longest title

Â  Â  Â  Â  Â  Â  return max(positions, key=len)

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return positions[0]

Â  Â Â 

Â  Â  def _merge_companies(self, companies: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple company names"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer the most complete company name

Â  Â  Â  Â  Â  Â  return max(companies, key=len)

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return companies[0]

Â  Â Â 

Â  Â  def _merge_locations(self, locations: List[str]) -> str:

Â  Â  Â  Â  """Merge multiple locations"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Prefer the most specific location (city, state/country)

Â  Â  Â  Â  Â  Â  return max(locations, key=lambda l: l.count(',') + len(l))

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return locations[0]

Â  Â Â 

Â  Â  def _merge_summaries(self, summaries: List[Optional[str]]) -> Optional[str]:

Â  Â  Â  Â  """Merge multiple summaries"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  valid_summaries = [s for s in summaries if s and s.strip()]

Â  Â  Â  Â  Â  Â  if not valid_summaries:

Â  Â  Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Combine unique parts of summaries

Â  Â  Â  Â  Â  Â  combined_parts = []

Â  Â  Â  Â  Â  Â  for summary in valid_summaries:

Â  Â  Â  Â  Â  Â  Â  Â  if summary not in combined_parts:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combined_parts.append(summary)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Limit combined length

Â  Â  Â  Â  Â  Â  combined = " | ".join(combined_parts)

Â  Â  Â  Â  Â  Â  return combined[:500] + "..." if len(combined) > 500 else combined

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return summaries[0] if summaries else None

Â  Â Â 

Â  Â  def _merge_sources(self, sources: List[Optional[str]]) -> Optional[str]:

Â  Â  Â  Â  """Merge multiple sources"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  valid_sources = [s for s in sources if s and s.strip()]

Â  Â  Â  Â  Â  Â  if not valid_sources:

Â  Â  Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  unique_sources = list(set(valid_sources))

Â  Â  Â  Â  Â  Â  return ", ".join(unique_sources)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return sources[0] if sources else None

Â  Â Â 

Â  Â  def _calculate_merged_confidence(self, contacts: List[ContactResult]) -> float:

Â  Â  Â  Â  """Calculate confidence for merged contact"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not contacts:

Â  Â  Â  Â  Â  Â  Â  Â  return 0.0

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Base confidence is the highest individual confidence

Â  Â  Â  Â  Â  Â  max_confidence = max(c.confidence_score for c in contacts)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Bonus for having multiple sources

Â  Â  Â  Â  Â  Â  source_bonus = min(0.1 * (len(contacts) - 1), 0.2)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Bonus for having complete information

Â  Â  Â  Â  Â  Â  merged_contact_data = {

Â  Â  Â  Â  Â  Â  Â  Â  'email': any(c.email for c in contacts),

Â  Â  Â  Â  Â  Â  Â  Â  'phone': any(c.phone for c in contacts),

Â  Â  Â  Â  Â  Â  Â  Â  'linkedin': any(c.linkedin_url for c in contacts),

Â  Â  Â  Â  Â  Â  Â  Â  'company': any(c.company for c in contacts),

Â  Â  Â  Â  Â  Â  Â  Â  'position': any(c.position for c in contacts)

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  completeness_bonus = sum(0.02 for v in merged_contact_data.values() if v)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  final_confidence = min(max_confidence + source_bonus + completeness_bonus, 1.0)

Â  Â  Â  Â  Â  Â  return final_confidence

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return max(c.confidence_score for c in contacts) if contacts else 0.0

Â  Â Â 

Â  Â  def _final_dedup_validation(self, contacts: List[ContactResult]) -> List[ContactResult]:

Â  Â  Â  Â  """Final validation to catch any remaining duplicates"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Quick check for exact matches that might have been missed

Â  Â  Â  Â  Â  Â  seen_emails = set()

Â  Â  Â  Â  Â  Â  seen_linkedin = set()

Â  Â  Â  Â  Â  Â  final_contacts = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for contact in contacts:

Â  Â  Â  Â  Â  Â  Â  Â  skip = False

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Check email

Â  Â  Â  Â  Â  Â  Â  Â  if contact.email:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  email_normalized = contact.email.lower().strip()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if email_normalized in seen_emails:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  skip = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen_emails.add(email_normalized)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  # Check LinkedIn URL

Â  Â  Â  Â  Â  Â  Â  Â  if contact.linkedin_url and not skip:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linkedin_normalized = self._normalize_linkedin_url(contact.linkedin_url)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if linkedin_normalized in seen_linkedin:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  skip = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen_linkedin.add(linkedin_normalized)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if not skip:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_contacts.append(contact)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return final_contacts

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Final validation failed: {e}")

Â  Â  Â  Â  Â  Â  return contacts

Â  Â Â 

Â  Â  def _basic_deduplicate(self, results: List[ContactResult]) -> List[ContactResult]:

Â  Â  Â  Â  """Fallback basic deduplication method"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  seen = set()

Â  Â  Â  Â  Â  Â  unique_results = []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for result in results:

Â  Â  Â  Â  Â  Â  Â  Â  # Create identifier for deduplication

Â  Â  Â  Â  Â  Â  Â  Â  identifier_parts = []

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if result.email:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  identifier_parts.append(result.email.lower())

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if result.name:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  identifier_parts.append(result.name.lower())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if result.company:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  identifier_parts.append(result.company.lower())

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  identifier = "|".join(identifier_parts)

Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  if identifier and identifier not in seen:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  seen.add(identifier)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unique_results.append(result)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return unique_results

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Basic deduplication failed: {e}")

Â  Â  Â  Â  Â  Â  return results

Â  Â Â 

Â  Â  # Normalization helper methods

Â  Â  def _normalize_name(self, name: str) -> str:

Â  Â  Â  Â  """Normalize name for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not name:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove extra whitespace, convert to lowercase

Â  Â  Â  Â  Â  Â  normalized = re.sub(r'\s+', ' ', name.strip().lower())

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove common prefixes/suffixes

Â  Â  Â  Â  Â  Â  prefixes = ['mr.', 'mrs.', 'ms.', 'dr.', 'prof.']

Â  Â  Â  Â  Â  Â  suffixes = ['jr.', 'sr.', 'ii', 'iii', 'iv']

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  words = normalized.split()

Â  Â  Â  Â  Â  Â  words = [w for w in words if w not in prefixes and w not in suffixes]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return ' '.join(words)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return name.lower() if name else ""

Â  Â Â 

Â  Â  def _normalize_company_name(self, company: str) -> str:

Â  Â  Â  Â  """Normalize company name for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not company:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  normalized = company.lower().strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove common company suffixes

Â  Â  Â  Â  Â  Â  suffixes = ['inc.', 'inc', 'corp.', 'corp', 'ltd.', 'ltd', 'llc', 'llp',Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'co.', 'co', 'company', 'corporation', 'limited']

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for suffix in suffixes:

Â  Â  Â  Â  Â  Â  Â  Â  if normalized.endswith(' ' + suffix):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normalized = normalized[:-len(suffix)-1].strip()

Â  Â  Â  Â  Â  Â  Â  Â  elif normalized.endswith(suffix):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  normalized = normalized[:-len(suffix)].strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove extra whitespace

Â  Â  Â  Â  Â  Â  normalized = re.sub(r'\s+', ' ', normalized)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return normalized

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return company.lower() if company else ""

Â  Â Â 

Â  Â  def _normalize_position(self, position: str) -> str:

Â  Â  Â  Â  """Normalize position title for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not position:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  normalized = position.lower().strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove common words that don't affect meaning

Â  Â  Â  Â  Â  Â  noise_words = ['senior', 'junior', 'lead', 'principal', 'chief', 'head of']

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  for word in noise_words:

Â  Â  Â  Â  Â  Â  Â  Â  normalized = normalized.replace(word, '').strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Normalize common abbreviations

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('mgr', 'manager')

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('eng', 'engineer')

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('dev', 'developer')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove extra whitespace

Â  Â  Â  Â  Â  Â  normalized = re.sub(r'\s+', ' ', normalized)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return normalized

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return position.lower() if position else ""

Â  Â Â 

Â  Â  def _normalize_location(self, location: str) -> str:

Â  Â  Â  Â  """Normalize location for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not location:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  normalized = location.lower().strip()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove common abbreviations and standardize

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('usa', 'united states')

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('uk', 'united kingdom')

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('ca', 'california')

Â  Â  Â  Â  Â  Â  normalized = normalized.replace('ny', 'new york')

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove extra whitespace

Â  Â  Â  Â  Â  Â  normalized = re.sub(r'\s+', ' ', normalized)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return normalized

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return location.lower() if location else ""

Â  Â Â 

Â  Â  def _normalize_linkedin_url(self, url: str) -> str:

Â  Â  Â  Â  """Normalize LinkedIn URL for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not url:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Extract the profile ID from LinkedIn URL

Â  Â  Â  Â  Â  Â  match = re.search(r'/in/([^/?]+)', url.lower())

Â  Â  Â  Â  Â  Â  return match.group(1) if match else url.lower()

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return url.lower() if url else ""

Â  Â Â 

Â  Â  def _normalize_phone(self, phone: str) -> str:

Â  Â  Â  Â  """Normalize phone number for comparison"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if not phone:

Â  Â  Â  Â  Â  Â  Â  Â  return ""

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Remove all non-digit characters

Â  Â  Â  Â  Â  Â  digits_only = re.sub(r'[^\d]', '', phone)

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Handle US numbers (remove country code if present)

Â  Â  Â  Â  Â  Â  if len(digits_only) == 11 and digits_only.startswith('1'):

Â  Â  Â  Â  Â  Â  Â  Â  digits_only = digits_only[1:]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return digits_only

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  except:

Â  Â  Â  Â  Â  Â  return phone if phone else ""

Â  Â Â 

Â  Â  async def close(self):

Â  Â  Â  Â  """Clean up resources"""

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  if self.browser_page:

Â  Â  Â  Â  Â  Â  Â  Â  self.browser_page.quit()

Â  Â  Â  Â  Â  Â  Â  Â  self.browser_page = None

Â  Â  Â  Â  Â  Â  Â  Â  self._browser_created = False

Â  Â  Â  Â  Â  Â  Â  Â  logger.info("âœ… Browser cleaned up")

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  logger.debug(f"âš ï¸ Cleanup error: {e}")



# Pydantic models for API

class SearchRequest(BaseModel):

Â  Â  industry: Optional[str] = Field(None, description="Target industry")

Â  Â  position: Optional[str] = Field(None, description="Job position/title")

Â  Â  company: Optional[str] = Field(None, description="Company name")

Â  Â  country: Optional[str] = Field(None, description="Country")

Â  Â  city: Optional[str] = Field(None, description="City")

Â  Â  keywords: Optional[str] = Field(None, description="Additional keywords")

Â  Â  experience_level: Optional[str] = Field(None, description="Experience level")

Â  Â  company_size: Optional[str] = Field(None, description="Company size")

Â  Â  max_results: int = Field(50, ge=1, le=200, description="Maximum results")

Â  Â  enable_deduplication: bool = Field(True, description="Enable advanced deduplication")

Â  Â  dedup_strictness: str = Field("medium", description="Deduplication strictness: loose, medium, strict")



class ContactResponse(BaseModel):

Â  Â  name: str

Â  Â  position: Optional[str] = None

Â  Â  company: Optional[str] = None

Â  Â  location: Optional[str] = None

Â  Â  email: Optional[str] = None

Â  Â  phone: Optional[str] = None

Â  Â  linkedin_url: Optional[str] = None

Â  Â  profile_url: Optional[str] = None

Â  Â  industry: Optional[str] = None

Â  Â  experience: Optional[str] = None

Â  Â  summary: Optional[str] = None

Â  Â  source: Optional[str] = None

Â  Â  scraped_at: Optional[str] = None

Â  Â  confidence_score: float



class DeduplicationRequest(BaseModel):

Â  Â  contacts: List[ContactResponse] = Field(..., description="List of contacts to deduplicate")

Â  Â  strictness: str = Field("medium", description="Deduplication strictness: loose, medium, strict")



class DeduplicationResponse(BaseModel):

Â  Â  success: bool

Â  Â  message: str

Â  Â  original_count: int

Â  Â  deduplicated_count: int

Â  Â  duplicates_removed: int

Â  Â  contacts: List[ContactResponse]

Â  Â  dedup_stats: dict



class SearchResponse(BaseModel):

Â  Â  success: bool

Â  Â  message: str

Â  Â  total_results: int

Â  Â  contacts: List[ContactResponse]

Â  Â  search_params: dict



# FastAPI Application

app = FastAPI(

Â  Â  title="Contact & People Scraper API",

Â  Â  description="Professional contact scraping service with headless browsing",

Â  Â  version="1.0.0"

)



# CORS Configuration

app.add_middleware(

Â  Â  CORSMiddleware,

Â  Â  allow_origins=[

Â  Â  Â  Â  "http://localhost:3000",Â  # Local development

Â  Â  Â  Â  "http://localhost:8080",Â  # Local frontend

Â  Â  Â  Â  "https://*.vercel.app",Â  Â # All Vercel deployments

Â  Â  Â  Â  "https://contact-scraper-frontend.vercel.app",Â  # Your specific Vercel URL (update after deployment)

Â  Â  Â  Â  os.getenv("FRONTEND_URL", ""),Â  # Environment variable

Â  Â  Â  Â  "*"Â  # Keep this for development, remove in production if needed

Â  Â  ],

Â  Â  allow_credentials=True,

Â  Â  allow_methods=["*"],

Â  Â  allow_headers=["*"],

)



# Global scraper instance with deduplication

scraper = ContactScraper(headless=True, stealth=True, dedup_strictness="medium")



@app.on_event("startup")

async def startup_event():

Â  Â  """Initialize scraper on startup"""

Â  Â  logger.info("ğŸš€ Contact Scraper API starting up...")



@app.on_event("shutdown")

async def shutdown_event():

Â  Â  """Clean up on shutdown"""

Â  Â  await scraper.close()

Â  Â  logger.info("ğŸ‘‹ Contact Scraper API shutting down...")



@app.get("/")

async def root():

Â  Â  """Railway health check endpoint"""

Â  Â  return {

Â  Â  Â  Â  "message": "Contact & People Scraper API",

Â  Â  Â  Â  "status": "healthy",

Â  Â  Â  Â  "environment": RAILWAY_ENV,

Â  Â  Â  Â  "version": "1.0.0"

Â  Â  }

Â  Â Â 

@app.get("/health")

async def health_check():

Â  Â  """Health check endpoint"""

Â  Â  return {

Â  Â  Â  Â  "status": "healthy",

Â  Â  Â  Â  "timestamp": datetime.now().isoformat(),

Â  Â  Â  Â  "scraper_ready": scraper._browser_created

Â  Â  }



@app.post("/search", response_model=SearchResponse)

async def search_contacts(request: SearchRequest):

Â  Â  """

Â  Â  Search for contacts based on provided parameters

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  # Convert request to search parameters

Â  Â  Â  Â  search_params = SearchParameters(

Â  Â  Â  Â  Â  Â  industry=request.industry,

Â  Â  Â  Â  Â  Â  position=request.position,

Â  Â  Â  Â  Â  Â  company=request.company,

Â  Â  Â  Â  Â  Â  country=request.country,

Â  Â  Â  Â  Â  Â  city=request.city,

Â  Â  Â  Â  Â  Â  keywords=request.keywords,

Â  Â  Â  Â  Â  Â  experience_level=request.experience_level,

Â  Â  Â  Â  Â  Â  company_size=request.company_size,

Â  Â  Â  Â  Â  Â  max_results=request.max_results

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Perform search

Â  Â  Â  Â  results = await scraper.search_contacts(search_params, request.enable_deduplication)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert results to response format

Â  Â  Â  Â  contact_responses = [

Â  Â  Â  Â  Â  Â  ContactResponse(**asdict(contact)) for contact in results

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  return SearchResponse(

Â  Â  Â  Â  Â  Â  success=True,

Â  Â  Â  Â  Â  Â  message=f"Found {len(results)} contacts (deduplication: {'enabled' if request.enable_deduplication else 'disabled'})",

Â  Â  Â  Â  Â  Â  total_results=len(results),

Â  Â  Â  Â  Â  Â  contacts=contact_responses,

Â  Â  Â  Â  Â  Â  search_params=asdict(search_params)

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"âŒ Search failed: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")



@app.post("/search/linkedin", response_model=SearchResponse)

async def search_linkedin_only(request: SearchRequest):

Â  Â  """

Â  Â  Search for contacts specifically on LinkedIn

Â  Â  WARNING: Ensure compliance with LinkedIn Terms of Service

Â  Â  Consider using LinkedIn's official API for production use

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  logger.warning("âš ï¸ LinkedIn-only search requested - ensure ToS compliance!")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert request to search parameters

Â  Â  Â  Â  search_params = SearchParameters(

Â  Â  Â  Â  Â  Â  industry=request.industry,

Â  Â  Â  Â  Â  Â  position=request.position,

Â  Â  Â  Â  Â  Â  company=request.company,

Â  Â  Â  Â  Â  Â  country=request.country,

Â  Â  Â  Â  Â  Â  city=request.city,

Â  Â  Â  Â  Â  Â  keywords=request.keywords,

Â  Â  Â  Â  Â  Â  experience_level=request.experience_level,

Â  Â  Â  Â  Â  Â  company_size=request.company_size,

Â  Â  Â  Â  Â  Â  max_results=min(request.max_results, 20)Â  # Lower limit for LinkedIn

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Perform LinkedIn-specific search

Â  Â  Â  Â  results = await scraper._search_linkedin_carefully(search_params)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert results to response format

Â  Â  Â  Â  contact_responses = [

Â  Â  Â  Â  Â  Â  ContactResponse(**asdict(contact)) for contact in results

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  return SearchResponse(

Â  Â  Â  Â  Â  Â  success=True,

Â  Â  Â  Â  Â  Â  message=f"Found {len(results)} LinkedIn contacts (ToS compliant search)",

Â  Â  Â  Â  Â  Â  total_results=len(results),

Â  Â  Â  Â  Â  Â  contacts=contact_responses,

Â  Â  Â  Â  Â  Â  search_params=asdict(search_params)

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"âŒ LinkedIn search failed: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"LinkedIn search failed: {str(e)}")



@app.get("/legal/disclaimer")

async def legal_disclaimer():

Â  Â  """

Â  Â  Legal disclaimer and Terms of Service information

Â  Â  """

Â  Â  return {

Â  Â  Â  Â  "disclaimer": "IMPORTANT LEGAL NOTICE",

Â  Â  Â  Â  "message": "This scraping tool must be used in compliance with all applicable laws and website Terms of Service",

Â  Â  Â  Â  "linkedin_warning": {

Â  Â  Â  Â  Â  Â  "notice": "LinkedIn has strict Terms of Service regarding automated data collection",

Â  Â  Â  Â  Â  Â  "recommendation": "Use LinkedIn's official API for production applications",

Â  Â  Â  Â  Â  Â  "api_url": "https://developer.linkedin.com/",

Â  Â  Â  Â  Â  Â  "compliance": "Ensure you have proper authorization and respect rate limits"

Â  Â  Â  Â  },

Â  Â  Â  Â  "general_guidelines": [

Â  Â  Â  Â  Â  Â  "Only collect publicly available information",

Â  Â  Â  Â  Â  Â  "Respect robots.txt files and rate limits",

Â  Â  Â  Â  Â  Â  "Do not overwhelm target servers",

Â  Â  Â  Â  Â  Â  "Comply with GDPR and privacy regulations",

Â  Â  Â  Â  Â  Â  "Obtain proper consent for data collection",

Â  Â  Â  Â  Â  Â  "Use data only for legitimate business purposes"

Â  Â  Â  Â  ],

Â  Â  Â  Â  "user_responsibility": "Users are solely responsible for ensuring their use of this tool complies with all applicable laws and website terms of service"

Â  Â  }



@app.get("/linkedin/status")

async def linkedin_scraping_status():

Â  Â  """

Â  Â  Get current LinkedIn scraping status and recommendations

Â  Â  """

Â  Â  return {

Â  Â  Â  Â  "current_requests": scraper.linkedin_request_count,

Â  Â  Â  Â  "hourly_limit": scraper.linkedin_max_requests_per_hour,

Â  Â  Â  Â  "remaining_requests": max(0, scraper.linkedin_max_requests_per_hour - scraper.linkedin_request_count),

Â  Â  Â  Â  "rate_limit_delay": scraper.linkedin_delay,

Â  Â  Â  Â  "recommendation": "Consider using LinkedIn's official API for higher volume and more reliable access",

Â  Â  Â  Â  "api_alternatives": {

Â  Â  Â  Â  Â  Â  "linkedin_api": "https://developer.linkedin.com/",

Â  Â  Â  Â  Â  Â  "sales_navigator": "https://business.linkedin.com/sales-solutions/sales-navigator",

Â  Â  Â  Â  Â  Â  "recruiter": "https://business.linkedin.com/talent-solutions/recruiter"

Â  Â  Â  Â  },

Â  Â  Â  Â  "status": "operational" if scraper.linkedin_request_count < scraper.linkedin_max_requests_per_hour else "rate_limited"

Â  Â  }



@app.post("/deduplicate", response_model=DeduplicationResponse)

async def deduplicate_contacts(request: DeduplicationRequest):

Â  Â  """

Â  Â  Deduplicate a list of contacts using advanced algorithms

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  logger.info(f"ğŸ”„ Deduplication requested for {len(request.contacts)} contacts with {request.strictness} strictness")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert ContactResponse objects to ContactResult objects

Â  Â  Â  Â  contact_results = []

Â  Â  Â  Â  for contact_data in request.contacts:

Â  Â  Â  Â  Â  Â  contact_dict = contact_data.dict()

Â  Â  Â  Â  Â  Â  contact_result = ContactResult(**contact_dict)

Â  Â  Â  Â  Â  Â  contact_results.append(contact_result)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Perform deduplication

Â  Â  Â  Â  deduplicated_contacts, stats = await scraper.deduplicate_contacts(

Â  Â  Â  Â  Â  Â  contact_results,Â 

Â  Â  Â  Â  Â  Â  request.strictness

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert back to response format

Â  Â  Â  Â  deduplicated_responses = [

Â  Â  Â  Â  Â  Â  ContactResponse(**asdict(contact)) for contact in deduplicated_contacts

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  return DeduplicationResponse(

Â  Â  Â  Â  Â  Â  success=True,

Â  Â  Â  Â  Â  Â  message=f"Deduplication complete. Removed {stats.get('duplicates_removed', 0)} duplicates",

Â  Â  Â  Â  Â  Â  original_count=stats.get('original_count', 0),

Â  Â  Â  Â  Â  Â  deduplicated_count=stats.get('deduplicated_count', 0),

Â  Â  Â  Â  Â  Â  duplicates_removed=stats.get('duplicates_removed', 0),

Â  Â  Â  Â  Â  Â  contacts=deduplicated_responses,

Â  Â  Â  Â  Â  Â  dedup_stats=stats

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"âŒ Deduplication failed: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"Deduplication failed: {str(e)}")



@app.get("/deduplicate/config")

async def get_deduplication_config():

Â  Â  """

Â  Â  Get current deduplication configuration and available options

Â  Â  """

Â  Â  return {

Â  Â  Â  Â  "current_strictness": scraper.dedup_strictness,

Â  Â  Â  Â  "current_thresholds": scraper.dedup_thresholds,

Â  Â  Â  Â  "available_strictness_levels": {

Â  Â  Â  Â  Â  Â  "loose": {

Â  Â  Â  Â  Â  Â  Â  Â  "description": "More permissive - catches obvious duplicates only",

Â  Â  Â  Â  Â  Â  Â  Â  "use_case": "When you want to keep more contacts and remove only clear duplicates",

Â  Â  Â  Â  Â  Â  Â  Â  "thresholds": scraper._get_dedup_thresholds("loose")

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  "medium": {

Â  Â  Â  Â  Â  Â  Â  Â  "description": "Balanced approach - good for most use cases",

Â  Â  Â  Â  Â  Â  Â  Â  "use_case": "Default setting - balances duplicate detection with false positives",

Â  Â  Â  Â  Â  Â  Â  Â  "thresholds": scraper._get_dedup_thresholds("medium")

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  "strict": {

Â  Â  Â  Â  Â  Â  Â  Â  "description": "Very restrictive - only keeps contacts that are clearly different",

Â  Â  Â  Â  Â  Â  Â  Â  "use_case": "When you want maximum deduplication, even if some unique contacts are removed",

Â  Â  Â  Â  Â  Â  Â  Â  "thresholds": scraper._get_dedup_thresholds("strict")

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  },

Â  Â  Â  Â  "deduplication_criteria": [

Â  Â  Â  Â  Â  Â  "Exact email match (highest priority)",

Â  Â  Â  Â  Â  Â  "Exact LinkedIn URL match",

Â  Â  Â  Â  Â  Â  "Exact phone number match",

Â  Â  Â  Â  Â  Â  "Name + Company similarity",

Â  Â  Â  Â  Â  Â  "Email domain + Name similarity",

Â  Â  Â  Â  Â  Â  "Name + Position + Location similarity"

Â  Â  Â  Â  ]

Â  Â  }



@app.post("/deduplicate/config")

async def update_deduplication_config(strictness: str):

Â  Â  """

Â  Â  Update deduplication strictness level

Â  Â  """

Â  Â  valid_levels = ["loose", "medium", "strict"]

Â  Â Â 

Â  Â  if strictness not in valid_levels:

Â  Â  Â  Â  raise HTTPException(

Â  Â  Â  Â  Â  Â  status_code=400,Â 

Â  Â  Â  Â  Â  Â  detail=f"Invalid strictness level. Must be one of: {valid_levels}"

Â  Â  Â  Â  )

Â  Â Â 

Â  Â  old_strictness = scraper.dedup_strictness

Â  Â  scraper.set_dedup_strictness(strictness)

Â  Â Â 

Â  Â  return {

Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  "message": f"Deduplication strictness updated from {old_strictness} to {strictness}",

Â  Â  Â  Â  "old_strictness": old_strictness,

Â  Â  Â  Â  "new_strictness": strictness,

Â  Â  Â  Â  "new_thresholds": scraper.dedup_thresholds

Â  Â  }



@app.get("/deduplicate/test")

async def test_deduplication():

Â  Â  """

Â  Â  Test deduplication with sample data

Â  Â  """

Â  Â  # Create sample duplicate contacts for testing

Â  Â  sample_contacts = [

Â  Â  Â  Â  ContactResult(

Â  Â  Â  Â  Â  Â  name="John Smith",

Â  Â  Â  Â  Â  Â  position="Software Engineer",

Â  Â  Â  Â  Â  Â  company="Tech Corp",

Â  Â  Â  Â  Â  Â  email="john.smith@techcorp.com",

Â  Â  Â  Â  Â  Â  location="San Francisco, CA",

Â  Â  Â  Â  Â  Â  source="Test Data",

Â  Â  Â  Â  Â  Â  confidence_score=0.9

Â  Â  Â  Â  ),

Â  Â  Â  Â  ContactResult(

Â  Â  Â  Â  Â  Â  name="J. Smith",

Â  Â  Â  Â  Â  Â  position="Senior Software Engineer",Â 

Â  Â  Â  Â  Â  Â  company="Tech Corp Inc.",

Â  Â  Â  Â  Â  Â  email="j.smith@techcorp.com",

Â  Â  Â  Â  Â  Â  location="San Francisco, California",

Â  Â  Â  Â  Â  Â  source="Test Data",

Â  Â  Â  Â  Â  Â  confidence_score=0.8

Â  Â  Â  Â  ),

Â  Â  Â  Â  ContactResult(

Â  Â  Â  Â  Â  Â  name="John D. Smith",

Â  Â  Â  Â  Â  Â  position="Software Developer",

Â  Â  Â  Â  Â  Â  company="Tech Corporation",

Â  Â  Â  Â  Â  Â  linkedin_url="https://linkedin.com/in/johnsmith123",

Â  Â  Â  Â  Â  Â  location="SF, CA",

Â  Â  Â  Â  Â  Â  source="Test Data",

Â  Â  Â  Â  Â  Â  confidence_score=0.85

Â  Â  Â  Â  ),

Â  Â  Â  Â  ContactResult(

Â  Â  Â  Â  Â  Â  name="Jane Doe",

Â  Â  Â  Â  Â  Â  position="Product Manager",

Â  Â  Â  Â  Â  Â  company="StartupXYZ",

Â  Â  Â  Â  Â  Â  email="jane@startupxyz.com",

Â  Â  Â  Â  Â  Â  source="Test Data",

Â  Â  Â  Â  Â  Â  confidence_score=0.7

Â  Â  Â  Â  ),

Â  Â  Â  Â  ContactResult(

Â  Â  Â  Â  Â  Â  name="Jane R. Doe",

Â  Â  Â  Â  Â  Â  position="Senior Product Manager",

Â  Â  Â  Â  Â  Â  company="StartupXYZ",

Â  Â  Â  Â  Â  Â  email="jane.doe@startupxyz.com",

Â  Â  Â  Â  Â  Â  location="New York, NY",

Â  Â  Â  Â  Â  Â  source="Test Data",

Â  Â  Â  Â  Â  Â  confidence_score=0.8

Â  Â  Â  Â  )

Â  Â  ]

Â  Â Â 

Â  Â  # Test with different strictness levels

Â  Â  results = {}

Â  Â Â 

Â  Â  for strictness in ["loose", "medium", "strict"]:

Â  Â  Â  Â  deduplicated, stats = await scraper.deduplicate_contacts(sample_contacts, strictness)

Â  Â  Â  Â  results[strictness] = {

Â  Â  Â  Â  Â  Â  "original_count": len(sample_contacts),

Â  Â  Â  Â  Â  Â  "deduplicated_count": len(deduplicated),

Â  Â  Â  Â  Â  Â  "duplicates_removed": len(sample_contacts) - len(deduplicated),

Â  Â  Â  Â  Â  Â  "duplicate_rate": round(((len(sample_contacts) - len(deduplicated)) / len(sample_contacts)) * 100, 2),

Â  Â  Â  Â  Â  Â  "contacts": [asdict(contact) for contact in deduplicated]

Â  Â  Â  Â  }

Â  Â Â 

Â  Â  return {

Â  Â  Â  Â  "test_description": "Sample deduplication test with 5 contacts containing 2 duplicate groups",

Â  Â  Â  Â  "sample_contacts": [asdict(contact) for contact in sample_contacts],

Â  Â  Â  Â  "results_by_strictness": results,

Â  Â  Â  Â  "recommendations": {

Â  Â  Â  Â  Â  Â  "loose": "Use when you want to keep maximum contacts",

Â  Â  Â  Â  Â  Â  "medium": "Recommended for most use cases",

Â  Â  Â  Â  Â  Â  "strict": "Use when duplicate removal is critical"

Â  Â  Â  Â  }

Â  Â  }



@app.post("/search/async")

async def search_contacts_async(request: SearchRequest, background_tasks: BackgroundTasks):

Â  Â  """

Â  Â  Start asynchronous contact search (for long-running searches)

Â  Â  """

Â  Â  # Generate search ID

Â  Â  search_id = hashlib.md5(f"{datetime.now().isoformat()}{request}".encode()).hexdigest()[:8]

Â  Â Â 

Â  Â  # Start background task

Â  Â  background_tasks.add_task(perform_background_search, search_id, request)

Â  Â Â 

Â  Â  return {

Â  Â  Â  Â  "search_id": search_id,

Â  Â  Â  Â  "status": "started",

Â  Â  Â  Â  "message": "Search started in background",

Â  Â  Â  Â  "check_url": f"/search/status/{search_id}"

Â  Â  }



# Background search storage (in production, use Redis or database)

search_results = {}



async def perform_background_search(search_id: str, request: SearchRequest):

Â  Â  """Perform search in background"""

Â  Â  try:

Â  Â  Â  Â  search_results[search_id] = {"status": "running", "progress": 0}

Â  Â  Â  Â Â 

Â  Â  Â  Â  search_params = SearchParameters(

Â  Â  Â  Â  Â  Â  industry=request.industry,

Â  Â  Â  Â  Â  Â  position=request.position,

Â  Â  Â  Â  Â  Â  company=request.company,

Â  Â  Â  Â  Â  Â  country=request.country,

Â  Â  Â  Â  Â  Â  city=request.city,

Â  Â  Â  Â  Â  Â  keywords=request.keywords,

Â  Â  Â  Â  Â  Â  experience_level=request.experience_level,

Â  Â  Â  Â  Â  Â  company_size=request.company_size,

Â  Â  Â  Â  Â  Â  max_results=request.max_results

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  results = await scraper.search_contacts(search_params)

Â  Â  Â  Â Â 

Â  Â  Â  Â  search_results[search_id] = {

Â  Â  Â  Â  Â  Â  "status": "completed",

Â  Â  Â  Â  Â  Â  "progress": 100,

Â  Â  Â  Â  Â  Â  "results": [asdict(contact) for contact in results],

Â  Â  Â  Â  Â  Â  "total_results": len(results)

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  search_results[search_id] = {

Â  Â  Â  Â  Â  Â  "status": "failed",

Â  Â  Â  Â  Â  Â  "error": str(e)

Â  Â  Â  Â  }



@app.get("/search/status/{search_id}")

async def get_search_status(search_id: str):

Â  Â  """Get status of background search"""

Â  Â  if search_id not in search_results:

Â  Â  Â  Â  raise HTTPException(status_code=404, detail="Search ID not found")

Â  Â Â 

Â  Â  return search_results[search_id]



@app.get("/api/stats")

async def get_api_stats():

Â  Â  """Get API usage statistics"""

Â  Â  return {

Â  Â  Â  Â  "linkedin_requests": scraper.linkedin_request_count,

Â  Â  Â  Â  "linkedin_limit": scraper.linkedin_max_requests_per_hour,

Â  Â  Â  Â  "browser_status": "active" if scraper._browser_created else "inactive",

Â  Â  Â  Â  "dedup_strictness": scraper.dedup_strictness,

Â  Â  Â  Â  "background_searches": len(search_results),

Â  Â  Â  Â  "uptime": datetime.now().isoformat(),

Â  Â  Â  Â  "version": "1.0.0"

Â  Â  }



@app.get("/api/clear-cache")

async def clear_cache():

Â  Â  """Clear background search cache"""

Â  Â  global search_results

Â  Â  old_count = len(search_results)

Â  Â  search_results = {}

Â  Â Â 

Â  Â  return {

Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  "message": f"Cleared {old_count} cached search results",

Â  Â  Â  Â  "remaining_results": len(search_results)

Â  Â  }



@app.post("/api/test-browser")

async def test_browser():

Â  Â  """Test browser functionality"""

Â  Â  try:

Â  Â  Â  Â  success = await scraper._create_browser()

Â  Â  Â  Â Â 

Â  Â  Â  Â  if success:

Â  Â  Â  Â  Â  Â  # Test basic functionality

Â  Â  Â  Â  Â  Â  scraper.browser_page.get("https://httpbin.org/user-agent")

Â  Â  Â  Â  Â  Â  user_agent = scraper.browser_page.html

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  Â  Â  "message": "Browser test successful",

Â  Â  Â  Â  Â  Â  Â  Â  "browser_created": scraper._browser_created,

Â  Â  Â  Â  Â  Â  Â  Â  "test_url": "https://httpbin.org/user-agent",

Â  Â  Â  Â  Â  Â  Â  Â  "response_length": len(user_agent) if user_agent else 0

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  "success": False,

Â  Â  Â  Â  Â  Â  Â  Â  "message": "Browser creation failed",

Â  Â  Â  Â  Â  Â  Â  Â  "browser_created": False

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": False,

Â  Â  Â  Â  Â  Â  "message": f"Browser test failed: {str(e)}",

Â  Â  Â  Â  Â  Â  "browser_created": scraper._browser_created

Â  Â  Â  Â  }



@app.get("/api/browser/restart")

async def restart_browser():

Â  Â  """Restart browser instance"""

Â  Â  try:

Â  Â  Â  Â  # Close existing browser

Â  Â  Â  Â  await scraper.close()

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Create new browser

Â  Â  Â  Â  success = await scraper._create_browser()

Â  Â  Â  Â Â 

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": success,

Â  Â  Â  Â  Â  Â  "message": "Browser restarted successfully" if success else "Browser restart failed",

Â  Â  Â  Â  Â  Â  "browser_status": "active" if success else "inactive"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": False,

Â  Â  Â  Â  Â  Â  "message": f"Browser restart failed: {str(e)}",

Â  Â  Â  Â  Â  Â  "browser_status": "error"

Â  Â  Â  Â  }



# Advanced search endpoints

@app.post("/search/advanced")

async def advanced_search(request: SearchRequest):

Â  Â  """

Â  Â  Advanced search with enhanced filtering and processing

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  logger.info(f"ğŸ” Advanced search requested: {request}")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert request to search parameters

Â  Â  Â  Â  search_params = SearchParameters(

Â  Â  Â  Â  Â  Â  industry=request.industry,

Â  Â  Â  Â  Â  Â  position=request.position,

Â  Â  Â  Â  Â  Â  company=request.company,

Â  Â  Â  Â  Â  Â  country=request.country,

Â  Â  Â  Â  Â  Â  city=request.city,

Â  Â  Â  Â  Â  Â  keywords=request.keywords,

Â  Â  Â  Â  Â  Â  experience_level=request.experience_level,

Â  Â  Â  Â  Â  Â  company_size=request.company_size,

Â  Â  Â  Â  Â  Â  max_results=request.max_results

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Perform search with enhanced processing

Â  Â  Â  Â  results = await scraper.search_contacts(search_params, request.enable_deduplication)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Enhanced result processing

Â  Â  Â  Â  enhanced_results = []

Â  Â  Â  Â  for contact in results:

Â  Â  Â  Â  Â  Â  # Add email pattern generation

Â  Â  Â  Â  Â  Â  if contact.name and contact.company and not contact.email:

Â  Â  Â  Â  Â  Â  Â  Â  email_patterns = scraper._generate_email_patterns(contact.name, contact.company)

Â  Â  Â  Â  Â  Â  Â  Â  contact.summary = f"Suggested emails: {', '.join(email_patterns[:3])}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  enhanced_results.append(contact)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Generate search analytics

Â  Â  Â  Â  analytics = {

Â  Â  Â  Â  Â  Â  "source_breakdown": {},

Â  Â  Â  Â  Â  Â  "confidence_stats": {

Â  Â  Â  Â  Â  Â  Â  Â  "high": len([c for c in enhanced_results if c.confidence_score >= 0.8]),

Â  Â  Â  Â  Â  Â  Â  Â  "medium": len([c for c in enhanced_results if 0.5 <= c.confidence_score < 0.8]),

Â  Â  Â  Â  Â  Â  Â  Â  "low": len([c for c in enhanced_results if c.confidence_score < 0.5])

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  "data_completeness": {

Â  Â  Â  Â  Â  Â  Â  Â  "with_email": len([c for c in enhanced_results if c.email]),

Â  Â  Â  Â  Â  Â  Â  Â  "with_phone": len([c for c in enhanced_results if c.phone]),

Â  Â  Â  Â  Â  Â  Â  Â  "with_linkedin": len([c for c in enhanced_results if c.linkedin_url]),

Â  Â  Â  Â  Â  Â  Â  Â  "with_location": len([c for c in enhanced_results if c.location])

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Count by source

Â  Â  Â  Â  for contact in enhanced_results:

Â  Â  Â  Â  Â  Â  source = contact.source or "Unknown"

Â  Â  Â  Â  Â  Â  analytics["source_breakdown"][source] = analytics["source_breakdown"].get(source, 0) + 1

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert results to response format

Â  Â  Â  Â  contact_responses = [

Â  Â  Â  Â  Â  Â  ContactResponse(**asdict(contact)) for contact in enhanced_results

Â  Â  Â  Â  ]

Â  Â  Â  Â Â 

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  "message": f"Advanced search completed. Found {len(enhanced_results)} contacts",

Â  Â  Â  Â  Â  Â  "total_results": len(enhanced_results),

Â  Â  Â  Â  Â  Â  "contacts": contact_responses,

Â  Â  Â  Â  Â  Â  "search_params": asdict(search_params),

Â  Â  Â  Â  Â  Â  "analytics": analytics,

Â  Â  Â  Â  Â  Â  "search_quality": {

Â  Â  Â  Â  Â  Â  Â  Â  "avg_confidence": sum(c.confidence_score for c in enhanced_results) / len(enhanced_results) if enhanced_results else 0,

Â  Â  Â  Â  Â  Â  Â  Â  "data_richness": sum(1 for c in enhanced_results if any([c.email, c.phone, c.linkedin_url])) / len(enhanced_results) if enhanced_results else 0

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"âŒ Advanced search failed: {e}")

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"Advanced search failed: {str(e)}")



# Export endpoints

@app.post("/export/csv")

async def export_contacts_csv(contacts: List[ContactResponse]):

Â  Â  """

Â  Â  Export contacts to CSV format

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  import csv

Â  Â  Â  Â  import io

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Create CSV in memory

Â  Â  Â  Â  output = io.StringIO()

Â  Â  Â  Â  writer = csv.DictWriter(output, fieldnames=[

Â  Â  Â  Â  Â  Â  'name', 'position', 'company', 'location', 'email', 'phone',

Â  Â  Â  Â  Â  Â  'linkedin_url', 'industry', 'source', 'confidence_score'

Â  Â  Â  Â  ])

Â  Â  Â  Â Â 

Â  Â  Â  Â  writer.writeheader()

Â  Â  Â  Â  for contact in contacts:

Â  Â  Â  Â  Â  Â  writer.writerow({

Â  Â  Â  Â  Â  Â  Â  Â  'name': contact.name,

Â  Â  Â  Â  Â  Â  Â  Â  'position': contact.position or '',

Â  Â  Â  Â  Â  Â  Â  Â  'company': contact.company or '',

Â  Â  Â  Â  Â  Â  Â  Â  'location': contact.location or '',

Â  Â  Â  Â  Â  Â  Â  Â  'email': contact.email or '',

Â  Â  Â  Â  Â  Â  Â  Â  'phone': contact.phone or '',

Â  Â  Â  Â  Â  Â  Â  Â  'linkedin_url': contact.linkedin_url or '',

Â  Â  Â  Â  Â  Â  Â  Â  'industry': contact.industry or '',

Â  Â  Â  Â  Â  Â  Â  Â  'source': contact.source or '',

Â  Â  Â  Â  Â  Â  Â  Â  'confidence_score': contact.confidence_score

Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â Â 

Â  Â  Â  Â  csv_content = output.getvalue()

Â  Â  Â  Â  output.close()

Â  Â  Â  Â Â 

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  "format": "csv",

Â  Â  Â  Â  Â  Â  "content": csv_content,

Â  Â  Â  Â  Â  Â  "records_exported": len(contacts),

Â  Â  Â  Â  Â  Â  "filename": f"contacts_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"CSV export failed: {str(e)}")



@app.post("/export/json")

async def export_contacts_json(contacts: List[ContactResponse]):

Â  Â  """

Â  Â  Export contacts to JSON format

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  export_data = {

Â  Â  Â  Â  Â  Â  "export_info": {

Â  Â  Â  Â  Â  Â  Â  Â  "timestamp": datetime.now().isoformat(),

Â  Â  Â  Â  Â  Â  Â  Â  "total_contacts": len(contacts),

Â  Â  Â  Â  Â  Â  Â  Â  "format": "json",

Â  Â  Â  Â  Â  Â  Â  Â  "version": "1.0.0"

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  "contacts": [contact.dict() for contact in contacts]

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  "format": "json",

Â  Â  Â  Â  Â  Â  "data": export_data,

Â  Â  Â  Â  Â  Â  "records_exported": len(contacts),

Â  Â  Â  Â  Â  Â  "filename": f"contacts_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"JSON export failed: {str(e)}")



# Validation endpoints

@app.post("/validate/email")

async def validate_email_patterns(contact: ContactResponse):

Â  Â  """

Â  Â  Generate and validate email patterns for a contact

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  if not contact.name or not contact.company:

Â  Â  Â  Â  Â  Â  raise HTTPException(status_code=400, detail="Name and company required for email pattern generation")

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Convert to ContactResult for processing

Â  Â  Â  Â  contact_result = ContactResult(**contact.dict())

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Generate email patterns

Â  Â  Â  Â  email_patterns = scraper._generate_email_patterns(contact.name, contact.company)

Â  Â  Â  Â Â 

Â  Â  Â  Â  # Basic email format validation

Â  Â  Â  Â  import re

Â  Â  Â  Â  email_regex = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}

Â  Â  Â  Â  valid_patterns = [email for email in email_patterns if re.match(email_regex, email)]

Â  Â  Â  Â Â 

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "success": True,

Â  Â  Â  Â  Â  Â  "contact_name": contact.name,

Â  Â  Â  Â  Â  Â  "company": contact.company,

Â  Â  Â  Â  Â  Â  "generated_patterns": email_patterns,

Â  Â  Â  Â  Â  Â  "valid_patterns": valid_patterns,

Â  Â  Â  Â  Â  Â  "pattern_count": len(valid_patterns),

Â  Â  Â  Â  Â  Â  "confidence": "medium" if len(valid_patterns) > 3 else "low"

Â  Â  Â  Â  }

Â  Â  Â  Â Â 

Â  Â  except Exception as e:

Â  Â  Â  Â  raise HTTPException(status_code=500, detail=f"Email validation failed: {str(e)}")



if __name__ == "__main__":

Â  Â  # Run the API server

Â  Â  uvicorn.run(

Â  Â  Â  Â  app,

Â  Â  Â  Â  host="0.0.0.0",

Â  Â  Â  Â  port=PORT,

Â  Â  Â  Â  reload=False,Â  # Disable in production

Â  Â  Â  Â  log_level="info"

Â  Â  )



# Example usage and API documentation:

"""

PROFESSIONAL CONTACT & PEOPLE SCRAPER API



ğŸš€ Features:

- Multi-source contact scraping (Google, LinkedIn, Business Directories, Company Websites)

- Advanced deduplication with fuzzy matching

- LinkedIn integration with ToS compliance warnings

- Railway deployment optimized

- Real-time browser automation with stealth features

- Comprehensive API with multiple endpoints



ğŸ“‹ Main Endpoints:

- POST /search - Main contact search

- POST /search/linkedin - LinkedIn-only search (use responsibly)

- POST /search/async - Background search for large requests

- POST /deduplicate - Advanced contact deduplication

- POST /search/advanced - Enhanced search with analytics

- GET /health - Health check

- GET /legal/disclaimer - Legal information



ğŸ”§ Advanced Features:

- Rate limiting and stealth browsing

- Email pattern generation

- Confidence scoring

- Source attribution

- Background search processing

- CSV/JSON export capabilities

- Browser management endpoints



âš–ï¸ Legal Compliance:

- LinkedIn ToS warnings and compliance features

- Rate limiting to respect target sites

- Public data only extraction

- User responsibility disclaimers



ğŸš€ Installation:

pip install DrissionPage fastapi uvicorn python-multipart



ğŸ”— Example Usage:

curl -X POST "https://your-railway-app.railway.app/search" \

-H "Content-Type: application/json" \

-d '{

Â  Â  "position": "Software Engineer",

Â  Â  "company": "Google",

Â  Â  "country": "United States",

Â  Â  "max_results": 10,

Â  Â  "enable_deduplication": true

}'



ğŸ“Š Advanced Search:

curl -X POST "https://your-railway-app.railway.app/search/advanced" \

-H "Content-Type: application/json" \

-d '{

Â  Â  "industry": "technology",

Â  Â  "position": "data scientist",

Â  Â  "experience_level": "senior",

Â  Â  "company_size": "large",

Â  Â  "max_results": 25

}'



ğŸ”„ Deduplication Only:

curl -X POST "https://your-railway-app.railway.app/deduplicate" \

-H "Content-Type: application/json" \

-d '{

Â  Â  "contacts": [/* your contact array */],

Â  Â  "strictness": "medium"

}'



âš ï¸ Important Notes:

1. Always respect website Terms of Service

2. Use LinkedIn's official API for production LinkedIn data

3. Implement proper rate limiting

4. Only collect publicly available information

5. Comply with GDPR and privacy regulations



ğŸŒ Frontend Integration:

The API is CORS-enabled and ready for frontend integration.

All endpoints return structured JSON responses.

"""
